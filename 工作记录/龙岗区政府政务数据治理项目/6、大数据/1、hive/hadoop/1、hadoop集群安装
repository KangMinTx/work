ä¸€ã€hadoopé›†ç¾¤å®‰è£…

1ã€å…ˆå®‰è£…vim å‘½ä»¤

æ‰§è¡Œ
yum -y install vim*


2ã€åœ¨è™šæ‹Ÿæœºæœºä¸­æ‰§è¡Œè¿™ä¸¤ä¸ªå‘½ä»¤
sudo yum install -y epel-release

sudo yum install -y psmisc nc net-tools rsync vim lrzsz ntp libzstd openssl-static tree iotop git

3ã€ä¿®æ”¹ip 

[chenbk@hadoop102 ~]$ vi /etc/sysconfig/network-scripts/ifcfg-ens33(vim ifcfg-eth0)

TYPE="Ethernet"
PROXY_METHOD="none"
BROWSER_ONLY="no"
BOOTPROTO="static"
DEFROUTE="yes"
IPV4_FAILURE_FATAL="no"
IPV6INIT="yes"
IPV6_AUTOCONF="yes"
IPV6_DEFROUTE="yes"
IPV6_FAILURE_FATAL="no"
IPV6_ADDR_GEN_MODE="stable-privacy"
NAME="ens33"
UUID="d02890fe-365c-46de-8830-5145dd4bfe3b"
DEVICE="ens33"
ONBOOT="yes"
IPADDR=192.168.250.102
GATEWAY=192.168.250.1
NETMASK=255.255.255.0
DNS1=8.8.8.8
DNS2=114.114.114.114


4ã€é‡å¯ç½‘ç»œé…ç½®
[chenbk@hadoop102 ~]$ systemctl restart network

5ã€ä¿®æ”¹ä¸»æœºåç§°ï¼ˆhadoop101ï¼‰

[chenbk@hadoop102 ~]$ vim /etc/hostname
hadoop102


6ã€æµ‹è¯•ç½‘ç»œ
[chenbk@hadoop102 ~]$ ping baidu.com

PING baidu.com (220.181.38.148) 56(84) bytes of data.
64 bytes from 220.181.38.148 (220.181.38.148): icmp_seq=1 ttl=52 time=37.9 ms
64 bytes from 220.181.38.148 (220.181.38.148): icmp_seq=2 ttl=52 time=37.6 ms
64 bytes from 220.181.38.148 (220.181.38.148): icmp_seq=3 ttl=52 time=37.8 ms
^C
7ã€ä¿®æ”¹hostsé…ç½®
[chenbk@hadoop102 ~]$ vim /etc/hosts
127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6
192.168.250.102 hadoop102
192.168.250.103 hadoop103
192.168.250.104 hadoop104
192.168.250.6   hadoop101
~
192.168.1.112 hadoop102
192.168.1.113 hadoop103
192.168.1.114 hadoop104
192.168.1.111 hadoop101
~
~

8ã€æœ¬åœ°macä¹Ÿè¦æ˜ å°„
1ã€æ‰“å¼€è®¿è¾¾
2ã€æŒ‰ä½ shift +æ§åˆ¶é”®+g
3ã€å‰å¾€  /etc/hosts
4ã€ä¿®æ”¹ä¸ºï¼š
##
# Host Database
#
# localhost is used to configure the loopback interface
# when the system is booting.  Do not change this entry.
##
127.0.0.1	localhost
255.255.255.255	broadcasthost
::1             localhost
127.0.0.1 www.sublimetext.com
127.0.0.1 license.sublimehq.com
192.168.250.102 hadoop102
192.168.250.103 hadoop103
192.168.250.104 hadoop104

9ã€åœ¨ç»ˆç«¯pingä¸€ä¸‹æ˜¯å¦è¿é€š
Ping hadoop102

10ã€å…³é—­é˜²ç«å¢™
sudo systemctl stop firewalld
sudo systemctl disable firewalld

11ã€åˆ›å»ºç”¨æˆ·å¹¶ç»™äºˆrootæƒé™
ï¼ˆsudo ï¼‰useradd chenbk 
ï¼ˆsudo ï¼‰passwd chenbk 
æˆ–è€…ç›´æ¥åœ¨è™šæ‹Ÿæœºä¸Šåˆ›å»º

ç”¨Rootæ‰§è¡Œï¼švisudo 

ä¿®æ”¹/etc/sudoersæ–‡ä»¶ï¼Œæ‰¾åˆ°ä¸‹é¢ä¸€è¡Œï¼ˆ91è¡Œï¼‰ï¼Œåœ¨rootä¸‹é¢æ·»åŠ ä¸€è¡Œï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š
     91 ## Allow root to run any commands anywhere
     92 root    ALL=(ALL)       ALL
     93 chenbk   ALL=(ALL)     ALL
åˆ©ç”¨å‘½ä»¤ï¼š
ï¼šset nu æ‰¾åˆ°91è¡Œ


12ã€åœ¨/optç›®å½•ä¸‹åˆ›å»ºæ–‡ä»¶å¤¹
ï¼ˆ1ï¼‰åœ¨/optç›®å½•ä¸‹åˆ›å»ºmoduleã€softwareæ–‡ä»¶å¤¹
sudo mkdir module
sudo mkdir software
ï¼ˆ2ï¼‰ä¿®æ”¹moduleã€softwareæ–‡ä»¶å¤¹çš„æ‰€æœ‰è€…cd 
sudo mkdir /opt/module /opt/software
sudo chown chenbk:chenbk /opt/module /opt/software


13ã€å®‰è£…JDKå’Œhadoop
1ã€åˆ©ç”¨shellå·¥å…·ï¼ŒæŠŠJDKå’Œhadoopå®‰è£…æ–‡ä»¶ï¼Œå¯¼å…¥åˆ° /opt/softwareæ–‡ä»¶å¤¹ä¸‹é¢
2ã€åœ¨Linuxç³»ç»Ÿä¸‹çš„optç›®å½•ä¸­æŸ¥çœ‹è½¯ä»¶åŒ…æ˜¯å¦å¯¼å…¥æˆåŠŸ
ls /opt/software/
3ã€çœ‹åˆ°çš„å†…å®¹
hadoop-3.1.3.tar.gz  jdk-8u212-linux-x64.tar.g
4ã€è§£å‹JDKåˆ°/opt/moduleç›®å½•ä¸‹ï¼ˆhadoopä¹Ÿä¸€æ ·ï¼‰
tar -zxvf jdk-8u212-linux-x64.tar.gz -C /opt/module/
tar -zxvf hadoop-3.1.3.tar.gz -C /opt/module/



14ã€é…ç½®JDKå’Œhadoopç¯å¢ƒå˜é‡
ï¼ˆ1ï¼‰æ–°å»º/etc/profile.d/my_env.shæ–‡ä»¶
sudo vim /etc/profile.d/my_env.sh
æ·»åŠ å¦‚ä¸‹å†…å®¹:

#JAVA_HOME
export JAVA_HOME=/opt/module/jdk1.8.0_212
export PATH=$PATH:$JAVA_HOME/bin
##HADOOP_HOME
export HADOOP_HOME=/opt/module/hadoop-3.1.3
export PATH=$PATH:$HADOOP_HOME/bin
export PATH=$PATH:$HADOOP_HOME/sbin

æœ€æ–°å¯æ·»åŠ ä»¥ä¸‹
#JAVA_HOME
export JAVA_HOME=/opt/module/jdk1.8.0_212
export PATH=$PATH:$JAVA_HOME/bin
##HADOOP_HOME
export HADOOP_HOME=/opt/module/hadoop-3.1.3
export PATH=$PATH:$HADOOP_HOME/bin
export PATH=$PATH:$HADOOP_HOME/sbin
#HIVE_HOME
export HIVE_HOME=/opt/module/hive
export PATH=$PATH:$HIVE_HOME/bin
#FLUME_HOME
export FLUME_HOME=/opt/module/flume
export PATH=$PATH:$FLUME_HOME/bin

#KAFKA_HOME
export KAFKA_HOME=/opt/module/kafka
export PATH=$PATH:$KAFKA_HOME/bin

#ZOOKEEPER_HOME
export ZOOKEEPER_HOME=/opt/module/zookeeper
export PATH=$PATH:$ZOOKEEPER_HOME/bin



2ï¼‰ä¿å­˜åé€€å‡º
3ï¼‰é‡å¯xshellçª—å£ï¼Œè®©ç¯å¢ƒå˜é‡ç”Ÿæ•ˆ
4ï¼‰è®©ä¿®æ”¹åçš„æ–‡ä»¶ç”Ÿæ•ˆ
[chenbk@ hadoop101 hadoop-3.1.3]$ source /etc/profile


15ã€æµ‹è¯•JDKå’Œhadoopæ˜¯å¦å®‰è£…æˆåŠŸ
[chenbk@hadoop102 ~]$ java -version
å¦‚ä¸‹ï¼š
openjdk version "1.8.0_131"
OpenJDK Runtime Environment (build 1.8.0_131-b12)
OpenJDK 64-Bit Server VM (build 25.131-b12, mixed mode

[chenbk@hadoop102 ~]$ hadoop version
å¦‚ä¸‹ï¼š
Hadoop 3.1.3
Source code repository https://gitbox.apache.org/repos/asf/hadoop.git -r ba631c436b806728f8ec2f54ab1e289526c90579
Compiled by ztang on 2019-09-12T02:47Z
Compiled with protoc 2.5.0
From source with checksum ec785077c385118ac91aadde5ec9799
This command was run using /opt/module/hadoop-3.1.3/share/hadoop/common/hadoop-common-3.1.3.jar


å†æ‰§è¡Œä¸€ä¸‹ï¼š
[chenbk@hadoop102 ~]$ hadoop checknative
å¦‚ä¸‹ï¼š
2021-03-16 04:25:39,727 INFO bzip2.Bzip2Factory: Successfully loaded & initialized native-bzip2 library system-native
2021-03-16 04:25:39,735 INFO zlib.ZlibFactory: Successfully loaded & initialized native-zlib library
2021-03-16 04:25:39,745 WARN erasurecode.ErasureCodeNative: ISA-L support is not available in your platform... using builtin-java codec where applicable
Native library checking:
hadoop:  true /opt/module/hadoop-3.1.3/lib/native/libhadoop.so.1.0.0
zlib:    true /lib64/libz.so.1
zstd  :  true /lib64/libzstd.so.1
snappy:  true /lib64/libsnappy.so.1
lz4:     true revision:10301
bzip2:   true /lib64/libbz2.so.1
openssl: true /lib64/libcrypto.so
ISA-L:   false libhadoop was built without ISA-L support

å³å®‰è£…å®Œæˆ


16ã€sshæ— å¯†ç™»å½•é…ç½®
1ã€ssh-keygen -t rsa
ç„¶åæ•²ï¼ˆyã€ä¸‰ä¸ªå›è½¦ï¼‰ï¼Œå°±ä¼šç”Ÿæˆä¸¤ä¸ªæ–‡ä»¶id_rsaï¼ˆç§é’¥ï¼‰ã€id_rsa.pubï¼ˆå…¬é’¥ï¼‰
å¦‚ä¸‹ï¼š
Generating public/private rsa key pair.
Enter file in which to save the key (/home/chenbk/.ssh/id_rsa): 
/home/chenbk/.ssh/id_rsa already exists.
Overwrite (y/n)? y
Enter passphrase (empty for no passphrase): 
Enter same passphrase again: 
Your identification has been saved in /home/chenbk/.ssh/id_rsa.
Your public key has been saved in /home/chenbk/.ssh/id_rsa.pub.
The key fingerprint is:
SHA256:982dO5krqPpHSY9dZK0jLXmto0zGWYvY2pfW2H1Q4lo chenbk@hadoop101
The key's randomart image is:
+---[RSA 2048]----+
|                .|
|               o.|
|             ooo |
|           .+ B.o|
|        S o+=O.* |
|         ..=B=E..|
|          .*o+oO=|
|          .o=.=+*|
|       .ooo  o.o+|
+----[SHA256]-----+


2ã€æ•²å‘½ä»¤æŸ¥çœ‹ï¼š
å¦‚ä¸‹ï¼š
[chenbk@hadoop102 ~]$ ll -a

drwx------. 6 chenbk chenbk  188 3æœˆ  16 04:17 .
drwxr-xr-x. 3 root   root     20 3æœˆ  12 23:48 ..
-rw-------. 1 chenbk chenbk 7368 3æœˆ  15 07:25 .bash_history
-rw-r--r--. 1 chenbk chenbk   18 8æœˆ   2 2017 .bash_logout
-rw-r--r--. 1 chenbk chenbk  193 8æœˆ   2 2017 .bash_profile
-rw-r--r--. 1 chenbk chenbk  231 8æœˆ   2 2017 .bashrc
drwxrwxr-x. 3 chenbk chenbk   18 1æœˆ  13 06:28 .cache
drwxrwxr-x. 3 chenbk chenbk   18 1æœˆ  13 06:28 .config
drwxr-xr-x. 4 chenbk chenbk   39 1æœˆ  13 06:00 .mozilla
drwx------. 2 chenbk chenbk   80 1æœˆ  14 04:44 .ssh
-rw-------. 1 chenbk chenbk 6558 3æœˆ  16 04:17 .viminfo
-rw-------. 1 chenbk chenbk  165 3æœˆ  13 18:51 .Xauthority
-rw-rw-r--. 1 chenbk chenbk  480 1æœˆ  14 05:07 xsync


3ã€æ•²å‘½ä»¤æŸ¥çœ‹å¯†é’¥
[chenbk@hadoop102 ~]$ cd .ssh
[chenbk@hadoop102 .ssh]$ ll
å¦‚ä¸‹ï¼š
æ€»ç”¨é‡ 16
-rw-------. 1 chenbk chenbk  398 1æœˆ  14 04:19 authorized_keysï¼ˆè¿™ä¸ªæ˜¯å·²ç»å‘ç»™è‡ªå·±çš„äº†ï¼‰
-rw-------. 1 chenbk chenbk 1679 1æœˆ  14 04:18 id_rsa
-rw-r--r--. 1 chenbk chenbk  398 1æœˆ  14 04:18 id_rsa.pub
-rw-r--r--. 1 chenbk chenbk  746 1æœˆ  14 04:44 known_hosts


ï¼ˆid_rsa ç§é’¥ï¼‰ã€ï¼ˆid_rsa.pub å…¬é’¥ï¼‰

4ã€å‘é€å…¬é’¥åˆ°ç›¸åº”çš„é›†ç¾¤ï¼ˆhadoop103ã€hadoop104ï¼‰
å…ˆå‘ç»™è‡ªå·±ï¼ˆhadoop101ï¼‰
å‘½ä»¤ï¼šssh-copy-id hadoop102
å†æŸ¥çœ‹ï¼Œä¼šå‡ºç°ï¼š authorized_keysï¼ˆå‘é€æˆåŠŸï¼‰

ç„¶åå†å‘ç»™ï¼ˆhadoop103ã€hadoop104ï¼‰
å‘½ä»¤ï¼šssh-copy-id hadoop103
å‘½ä»¤ï¼šssh-copy-id hadoop104

æœ€åç”¨hadoop101ç™»å½•ï¼ˆhadoop102ã€hadoop103ï¼‰
ssh hadoop102
ssh hadoop103
å‘ç°ä¸ç”¨å¯†ç å°±å¯ä»¥ç™»å½•
å³æˆåŠŸï¼
[chenbk@hadoop102 ~]$ ssh hadoop103
Last login: Wed Mar 17 07:02:33 2021 from 192.168.250.102
[chenbk@hadoop103 ~]$

17ã€ç¼–è¾‘xsyncå‘½ä»¤
åˆ›å»ºxsync è„šæœ¬
[chenbk@hadoop102 ~]$ vim xsync
æ·»åŠ è„šæœ¬ï¼š

#!/bin/bash
#æ ¡éªŒå‚æ•°æ˜¯å¦åˆæ³•
if(($#==0))
then
        echo è¯·è¾“å…¥è¦åˆ†å‘çš„æ–‡ä»¶!
        exit;
fi
#è·å–åˆ†å‘æ–‡ä»¶çš„ç»å¯¹è·¯å¾„
dirpath=$(cd `dirname $1`; pwd -P)
filename=`basename $1`

echo è¦åˆ†å‘çš„æ–‡ä»¶çš„è·¯å¾„æ˜¯:$dirpath/$filename

#å¾ªç¯æ‰§è¡Œrsyncåˆ†å‘æ–‡ä»¶åˆ°é›†ç¾¤çš„æ¯æ¡æœºå™¨
for((i=102;i<=104;i++))
do
        echo ---------------------hadoop$i---------------------
        rsync -rvlt $dirpath/$filename  chenbk@hadoop$i:$dirpath
done

--æ³¨æ„ï¼šForé‡Œé¢çš„(101ï¼Œ102ï¼Œ103)æ˜¯é›†ç¾¤çš„æœ€åä¸‰ä½

å¦å¤–ï¼Œè¦ç¾¤å‘çš„æ—¶å€™ï¼Œéœ€è¦
bash xsync /opt/module
bash xsync /opt/software/
bash xsync /opt/module/hadoop-3.1.3/etc/hadoop/
bash xsync /etc/profile.d/my_env.sh

18ã€hadoopé›†ç¾¤äº’é€š

 åŸç†ï¼šæŠŠhadoop101ä¸­çš„.sshæ–‡ä»¶ï¼Œå…¨éƒ¨å‘é€åˆ°ï¼ˆhadoop102.hadoop103ï¼‰ä¸­

å‘½ä»¤ï¼š
[chenbk@hadoop101 ~]$ xsync .ssh
==================== hadoop101 ====================
The authenticity of host 'hadoop101 (192.168.250.6)' can't be established.
ECDSA key fingerprint is SHA256:cdWguy1YifkxYpCG5O8LKkzqg2VAEbKxavUjrmT0o3s.
ECDSA key fingerprint is MD5:51:fd:bf:79:1e:c7:0f:f9:b8:4b:ea:13:4d:eb:d5:b2.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added 'hadoop101' (ECDSA) to the list of known hosts.
chenbk@hadoop101's password: 
chenbk@hadoop101's password: 
sending incremental file list

sent 179 bytes  received 17 bytes  17.04 bytes/sec
total size is 3,535  speedup is 18.04
==================== hadoop102 ====================
/etc/bashrc: ç¬¬ 84 è¡Œ:.: /etc/profile.d/my_env.sh: æ˜¯ä¸€ä¸ªç›®å½•
/etc/bashrc: ç¬¬ 84 è¡Œ:.: /etc/profile.d/my_env.sh: æ˜¯ä¸€ä¸ªç›®å½•
sending incremental file list
.ssh/
.ssh/authorized_keys
.ssh/id_rsa
.ssh/id_rsa.pub
.ssh/known_hosts

sent 3,889 bytes  received 114 bytes  8,006.00 bytes/sec
total size is 3,535  speedup is 0.88
==================== hadoop103 ====================
sending incremental file list
.ssh/
.ssh/authorized_keys
.ssh/id_rsa
.ssh/id_rsa.pub
.ssh/known_hosts

sent 3,889 bytes  received 138 bytes  8,054.00 bytes/sec
total size is 3,535  speedup is 0.88


19ã€é…ç½®é›†ç¾¤
å…ˆè¿›å…¥ï¼š[chenbk@hadoop102 ~]$ cd /opt/module/hadoop-3.1.3/etc/hadoop/
å†æ‰§è¡Œï¼š[chenbk@hadoop102 hadoop]$ vim core-site.xml
æ·»åŠ ä»¥ä¸‹ï¼š
<property>
        <name>fs.defaultFS</name>
        <value>hdfs://hadoop102:8020</value>
    </property>
    <property>
        <name>hadoop.data.dir</name>
        <value>/opt/module/hadoop-3.1.3/data</value>
    </property>
    <property>
        <name>hadoop.proxyuser.chenbk.hosts</name>
        <value>*</value>
    </property>
    <property>
        <name>hadoop.proxyuser.chenbk.groups</name>
        <value>*</value>
    </property>
    <property>
        <name>hadoop.http.staticuser.user</name>
        <value>chenbk</value>
    </property>

é…ç½®hdfs-site.xml
[chenbk@hadoop102 hadoop]$ vim hdfs-site.xml

æ·»åŠ ä»¥ä¸‹ï¼š
<property>
     <name>dfs.namenode.name.dir</name>
     <value>file://${hadoop.data.dir}/name</value>
</property>
<property>
     <name>dfs.datanode.data.dir</name>
     <value>file://${hadoop.data.dir}/date</value>
</property>
<property>
     <name>dfs.namenode.checkpoint.dir</name>
     <value>file://${hadoop.data.dir}/namesecondary</value>
</property>
<property>
     <name>dfs.client.datanode-restart.timeout</name>
     <value>30</value>
</property>
<property>
     <name>dfs.namenode.secondary.http-address</name>
     <value>hadoop104:8986</value>
</property>


YARNé…ç½®æ–‡ä»¶
[chenbk@hadoop102 hadoop]$ vim yarn-site.xml

æ·»åŠ ä»¥ä¸‹ï¼š
<property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
    <property>
        <name>yarn.resourcemanager.hostname</name>
        <value>hadoop103</value>
    </property>
    <property>
        <name>yarn.nodemanager.env-whitelist</name>
        <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME</value>
    </property>
<property>
    <name>yarn.log-aggregation-enable</name>
    <value>true</value>
</property>
<property>
    <name>yarn.log.server.url</name>
    <value>http://hadoop102:19888/jobhistory/logs</value>
</property>
<property>
    <name>yarn.log-aggregation.retain-seconds</name>
    <value>604800</value>
</property>


MapReduceé…ç½®æ–‡ä»¶
[chenbk@hadoop102 hadoop]$ vim mapred-site.xml
æ·»åŠ ä»¥ä¸‹ï¼š
<property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
<!-- å†å²æœåŠ¡å™¨ç«¯åœ°å€ -->
<property>
    <name>mapreduce.jobhistory.address</name>
    <value>hadoop102:10020</value>
</property>

<!-- å†å²æœåŠ¡å™¨webç«¯åœ°å€ -->
<property>
    <name>mapreduce.jobhistory.webapp.address</name>
    <value>hadoop102:19888</value>
</property>


é…ç½®workers
[chenbk@hadoop102 ~]$ vim /opt/module/hadoop-3.1.3/etc/hadoop/workers
æ·»åŠ ä»¥ä¸‹ï¼š
hadoop102
hadoop103
hadoop104


20ã€é›†ç¾¤ç¾¤å‘

[chenbk@hadoop102 ~]$ bash xsync /opt/module/hadoop-3.1.3/etc/hadoop/
è¦åˆ†å‘çš„æ–‡ä»¶çš„è·¯å¾„æ˜¯:/opt/module/hadoop-3.1.3/etc/hadoop

---------------------hadoop102---------------------
sending incremental file list

sent 879 bytes  received 18 bytes  1,794.00 bytes/sec
total size is 108,561  speedup is 121.03
---------------------hadoop103---------------------
sending incremental file list
hadoop/
hadoop/mapred-site.xml
hadoop/yarn-site.xml

sent 2,198 bytes  received 83 bytes  1,520.67 bytes/sec
total size is 108,561  speedup is 47.59
---------------------hadoop104---------------------
sending incremental file list
hadoop/
hadoop/mapred-site.xml
hadoop/yarn-site.xml


21ã€å¯åŠ¨é›†ç¾¤
ï¼ˆ1ï¼‰å¦‚æœé›†ç¾¤æ˜¯ç¬¬ä¸€æ¬¡å¯åŠ¨ï¼Œéœ€è¦åœ¨hadoop102èŠ‚ç‚¹æ ¼å¼åŒ–NameNodeï¼ˆæ³¨æ„æ ¼å¼åŒ–ä¹‹å‰ï¼Œä¸€å®šè¦å…ˆåœæ­¢ä¸Šæ¬¡å¯åŠ¨çš„æ‰€æœ‰namenodeå’Œdatanodeè¿›ç¨‹ï¼Œç„¶åå†åˆ é™¤dataå’Œlogæ•°æ®ï¼‰
 æ‰§è¡Œï¼šhdfs namenode -format

[chenbk@hadoop102 hadoop-3.1.3]$ sbin/start-dfs.sh
ï¼ˆ2ï¼‰å¯åŠ¨HDFS
sbin/start-dfs.sh
ç”±102å¯åŠ¨
ï¼ˆ3ï¼‰åœ¨é…ç½®äº†ResourceManagerçš„èŠ‚ç‚¹ï¼ˆhadoop103ï¼‰å¯åŠ¨YARN
sbin/start-yarn.sh
ç”±103å¯åŠ¨

3ã€å¯åŠ¨å†å²è®°å½•
mapred --daemon start historyserver
[chenbk@hadoop102 hadoop-3.1.3]$ mapred --daemon start historyserver
æŸ¥çœ‹å¯åŠ¨æƒ…å†µï¼šjps
åœæ­¢é›†ç¾¤ï¼šstop-dfs.sh
		stop-yarn.sh
æŸ¥çœ‹ï¼š 192.168.250.102:9870/dfshealth.html#tab-overview
http://192.168.250.103:8088/cluster/apps


http://192.168.1.112:9870/dfshealth.html#tab-datanode-volume-failures
http://192.168.1.113:8088/cluster/nodes

22ã€æŸ¥çœ‹è¿è¡ŒçŠ¶å†µï¼Œå†™jpsall è„šæœ¬
å…ˆæ‰§è¡Œ
[chenbk@hadoop102 ~]$ vim jpsall
å†™è„šæœ¬
#!/bin/bash
for i in hadoop102 hadoop103 hadoop104
do

  echo "=====  $i   ====="
  ssh $i "jps" | grep -v Jps
done
ä¿å­˜é€€å‡º
å†æ‰§è¡Œ
[chenbk@hadoop102 ~]$ chmod +x jpsall
æœ€åæ‰§è¡Œ ./jpsall
[chenbk@hadoop102 ~]$ vim jpsall 
[chenbk@hadoop102 ~]$ vim jpsall 
[chenbk@hadoop102 ~]$ chmod +x jpsall 
[chenbk@hadoop102 ~]$ ./jpsall 
=====  hadoop102   =====
53826 DataNode
54306 JobHistoryServer
53720 NameNode
54142 NodeManager
=====  hadoop103   =====
42277 DataNode
42458 ResourceManager
42589 NodeManager
=====  hadoop104   =====
40791 DataNode
41000 NodeManager
40894 SecondaryNameNode
[chenbk@hadoop102 ~]$

å†ç§»åŠ¨æ–‡ä»¶åˆ°binä¸­
æ‰§è¡Œ[chenbk@hadoop102 ~]$ sudo mv jpsall /bin
å†æ‰§è¡Œï¼š[chenbk@hadoop102 ~]$ jpsall
[chenbk@hadoop102 ~]$ jpsall 
=====  hadoop102   =====
53826 DataNode
54306 JobHistoryServer
53720 NameNode
54142 NodeManager
=====  hadoop103   =====
42277 DataNode
42458 ResourceManager
42589 NodeManager
=====  hadoop104   =====
40791 DataNode
41000 NodeManager
40894 SecondaryNameNode
[chenbk@hadoop102 ~]$


23ã€é›†ç¾¤æ—¶é—´åŒæ­¥
é¦–å…ˆæ£€æŸ¥æœ‰æ²¡æœ‰ntpæœåŠ¡
æ‰§è¡Œï¼šrpm -qa | grep ntp
[chenbk@hadoop102 ~]$ rpm -qa | grep ntp
ntp-4.2.6p5-29.el7.centos.2.x86_64
python-ntplib-0.3.2-1.el7.noarch
ntpdate-4.2.6p5-29.el7.centos.2.x86_64
fontpackages-filesystem-1.44-8.el7.noarch
æœ‰ntp 
æŸ¥çœ‹æœ‰æ²¡æœ‰è¿è¡Œ
æ‰§è¡Œï¼š[chenbk@hadoop102 ~]$ sudo systemctl status ntpd
â— ntpd.service - Network Time Service
   Loaded: loaded (/usr/lib/systemd/system/ntpd.service; disabled; vendor preset: disabled)
   Active: inactive (dead)
å‡ºç°deadï¼ˆåœæ­¢ï¼‰ä¸ºæ­£å¸¸ï¼Œæ‰€ä»¥ä¸‰å°éƒ½éœ€è¦æ­£å¸¸
æˆ–æ‰§è¡Œï¼š[chenbk@hadoop102 ~]$ sudo systemctl stop ntpd
[chenbk@hadoop102 ~]$ sudo systemctl disable ntpd

ç„¶ååˆ‡æ¢ç”¨æˆ·ï¼ˆå…¨éƒ¨ä¸ºrootï¼‰102ä¸ºæ—¶é—´ç”¨æˆ·
æ‰§è¡Œï¼š[root@hadoop102 ~]# vim /etc/ntp.conf
aï¼‰ä¿®æ”¹1ï¼ˆæˆæƒ192.168.1.0-192.168.1.255ç½‘æ®µä¸Šçš„æ‰€æœ‰æœºå™¨å¯ä»¥ä»è¿™å°æœºå™¨ä¸ŠæŸ¥è¯¢å’ŒåŒæ­¥æ—¶é—´ï¼‰å…¶ä¸­192.168.1.0 è¿™ä¸ª1è¦çœ‹ä¸ªäººçš„ç½‘å…³
#restrict 192.168.1.0 mask 255.255.255.0 nomodify notrap
ä¸ºrestrict 192.168.1.0 mask 255.255.255.0 nomodify notrap
	bï¼‰ä¿®æ”¹2ï¼ˆé›†ç¾¤åœ¨å±€åŸŸç½‘ä¸­ï¼Œä¸ä½¿ç”¨å…¶ä»–äº’è”ç½‘ä¸Šçš„æ—¶é—´ï¼‰
server 0.centos.pool.ntp.org iburst
server 1.centos.pool.ntp.org iburst
server 2.centos.pool.ntp.org iburst
server 3.centos.pool.ntp.org iburst
ä¸º
#server 0.centos.pool.ntp.org iburst
#server 1.centos.pool.ntp.org iburst
#server 2.centos.pool.ntp.org iburst
#server 3.centos.pool.ntp.org iburst
cï¼‰æ·»åŠ 3ï¼ˆå½“è¯¥èŠ‚ç‚¹ä¸¢å¤±ç½‘ç»œè¿æ¥ï¼Œä¾ç„¶å¯ä»¥é‡‡ç”¨æœ¬åœ°æ—¶é—´ä½œä¸ºæ—¶é—´æœåŠ¡å™¨ä¸ºé›†ç¾¤ä¸­çš„å…¶ä»–èŠ‚ç‚¹æä¾›æ—¶é—´åŒæ­¥ï¼‰
server 127.127.1.0
fudge 127.127.1.0 stratum 10

å®Œæˆåï¼Œå¦‚ä¸‹ï¼š
[root@hadoop102 ~]# vim /etc/ntp.conf

# For more information about this file, see the man pages
# ntp.conf(5), ntp_acc(5), ntp_auth(5), ntp_clock(5), ntp_misc(5), ntp_mon(5).

driftfile /var/lib/ntp/drift

# Permit time synchronization with our time source, but do not
# permit the source to query or modify the service on this system.
restrict default nomodify notrap nopeer noquery

# Permit all access over the loopback interface.  This could
# be tightened as well, but to do so would effect some of
# the administrative functions.
restrict 127.0.0.1
restrict ::1

# Hosts on local network are less restricted.
restrict 192.168.1.0 mask 255.255.255.0 nomodify notrap

# Use public servers from the pool.ntp.org project.
# Please consider joining the pool (http://www.pool.ntp.org/join.html).
#server 0.centos.pool.ntp.org iburst
#server 1.centos.pool.ntp.org iburst
#server 2.centos.pool.ntp.org iburst
#server 3.centos.pool.ntp.org iburst
server 127.127.1.0
fudge 127.127.1.0 stratum 10

#broadcast 192.168.1.255 autokey        # broadcast server
#broadcastclient                        # broadcast client
#broadcast 224.0.1.1 autokey            # multicast server
-- æ’å…¥ --                                                 
ä¿å­˜é€€å‡º
ï¼ˆ3ï¼‰ä¿®æ”¹/etc/sysconfig/ntpd æ–‡ä»¶
vim /etc/sysconfig/ntpd
å¢åŠ å†…å®¹å¦‚ä¸‹ï¼ˆè®©ç¡¬ä»¶æ—¶é—´ä¸ç³»ç»Ÿæ—¶é—´ä¸€èµ·åŒæ­¥ï¼‰
SYNC_HWCLOCK=yes

# Command line options for ntpd
OPTIONS="-g"
SYNC_HWCLOCK=yes
~                                                                                                  
~                 
ï¼ˆ4ï¼‰é‡æ–°å¯åŠ¨ntpdæœåŠ¡
systemctl start ntpd
ï¼ˆ5ï¼‰è®¾ç½®ntpdæœåŠ¡å¼€æœºå¯åŠ¨
systemctl enable ntpd

å†æ‰§è¡Œ[root@hadoop102 ~]# systemctl status ntpd
â— ntpd.service - Network Time Service
   Loaded: loaded (/usr/lib/systemd/system/ntpd.service; enabled; vendor preset: disabled)
   Active: active (running) since å…­ 2021-01-16 07:15:44 PST; 1min 4s ago
 Main PID: 59709 (ntpd)
   CGroup: /system.slice/ntpd.service
           â””â”€59709 /usr/sbin/ntpd -u ntp:ntp -g

1æœˆ 16 07:15:44 hadoop102 ntpd[59709]: Listen normally on 2 lo 127.0.0.1 UDP 123
1æœˆ 16 07:15:44 hadoop102 ntpd[59709]: Listen normally on 3 ens33 192.168.250.102 UDP 123
1æœˆ 16 07:15:44 hadoop102 ntpd[59709]: Listen normally on 4 virbr0 192.168.122.1 UDP 123
1æœˆ 16 07:15:44 hadoop102 ntpd[59709]: Listen normally on 5 lo ::1 UDP 123
1æœˆ 16 07:15:44 hadoop102 ntpd[59709]: Listen normally on 6 ens33 fe80::c45b:d118:167d:bbfd ...123
1æœˆ 16 07:15:44 hadoop102 ntpd[59709]: Listening on routing socket on fd #23 for interface updates
1æœˆ 16 07:15:44 hadoop102 ntpd[59709]: 0.0.0.0 c016 06 restart
1æœˆ 16 07:15:44 hadoop102 ntpd[59709]: 0.0.0.0 c012 02 freq_set kernel 0.000 PPM
1æœˆ 16 07:15:44 hadoop102 ntpd[59709]: 0.0.0.0 c011 01 freq_not_set
1æœˆ 16 07:15:45 hadoop102 ntpd[59709]: 0.0.0.0 c514 04 freq_mode
Hint: Some lines were ellipsized, use -l to show in full.

æˆåŠŸ

ç„¶ååœ¨103ä¸Šï¼ˆrootï¼‰
æ‰§è¡Œå¦‚ä¸‹
[root@hadoop103 ~]# crontab -e
æ’å…¥
*/3 * * * * /usr/sbin/ntpdate hadoop102

ä¿å­˜é€€å‡º
ç„¶ååœ¨104ä¸Šï¼ˆrootï¼‰
æ‰§è¡Œå¦‚ä¸‹
[root@hadoop103 ~]# crontab -e
æ’å…¥
*/3 * * * * /usr/sbin/ntpdate hadoop102

ä¿å­˜é€€å‡º

æµ‹è¯•
ä¿®æ”¹ä»»æ„ä¸€å°è®¾å¤‡çš„æ—¶é—´
æ‰§è¡Œå‘½ä»¤ï¼š
[root@hadoop103 ~]# date -s "2017-9-11 11:11:11"
2017å¹´ 09æœˆ 11æ—¥ æ˜ŸæœŸä¸€ 11:11:11 PDT

ä¸‰åˆ†é’Ÿè¿‡åï¼š
[root@hadoop103 ~]# date
2021å¹´ 01æœˆ 16æ—¥ æ˜ŸæœŸå…­ 07:28:19 PST
æ‚¨åœ¨ /var/spool/mail/root ä¸­æœ‰æ–°é‚®ä»¶
[root@hadoop103 ~]#
[root@hadoop103 ~]# date -s "2017-9-11 11:11:11"
2017å¹´ 09æœˆ 11æ—¥ æ˜ŸæœŸä¸€ 11:11:11 PDT
[root@hadoop103 ~]# date
2021å¹´ 01æœˆ 16æ—¥ æ˜ŸæœŸå…­ 07:28:19 PST
æ‚¨åœ¨ /var/spool/mail/root ä¸­æœ‰æ–°é‚®ä»¶
[root@hadoop103 ~]#
æŸ¥çœ‹é‚®ä»¶ï¼š
[root@hadoop104 ~]# cat $MAIL

æŸ¥çœ‹ç«¯å£
æ‰§è¡Œï¼š[root@hadoop102 ~]# netstat -nltp
æŸ¥çœ‹ç«¯å£æ•°é‡ï¼š
[root@hadoop102 ~]# netstat -nltp
Active Internet connections (only servers)
Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name    
tcp        0      0 0.0.0.0:9867            0.0.0.0:*               LISTEN      53826/java          
tcp        0      0 0.0.0.0:9870            0.0.0.0:*               LISTEN      53720/java          
tcp        0      0 0.0.0.0:111             0.0.0.0:*               LISTEN      1/systemd           
tcp        0      0 192.168.250.102:19888   0.0.0.0:*               LISTEN      54306/java          
tcp        0      0 0.0.0.0:10033           0.0.0.0:*               LISTEN      54306/java          
tcp        0      0 192.168.250.102:8020    0.0.0.0:*               LISTEN      53720/java          
tcp        0      0 192.168.122.1:53        0.0.0.0:*               LISTEN      1241/dnsmasq        
tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN      865/sshd            
tcp        0      0 127.0.0.1:631           0.0.0.0:*               LISTEN      862/cupsd           
tcp        0      0 127.0.0.1:25            0.0.0.0:*               LISTEN      1159/master         
tcp        0      0 0.0.0.0:13562           0.0.0.0:*               LISTEN      54142/java          
tcp        0      0 127.0.0.1:6011          0.0.0.0:*               LISTEN      58456/sshd: chenbk@ 
tcp        0      0 0.0.0.0:33824           0.0.0.0:*               LISTEN      54142/java          
tcp        0      0 192.168.250.102:10020   0.0.0.0:*               LISTEN      54306/java          
tcp        0      0 127.0.0.1:34821         0.0.0.0:*               LISTEN      53826/java          
tcp        0      0 0.0.0.0:8040            0.0.0.0:*               LISTEN      54142/java          
tcp        0      0 0.0.0.0:9864            0.0.0.0:*               LISTEN      53826/java          
tcp        0      0 0.0.0.0:8042            0.0.0.0:*               LISTEN      54142/java          
tcp        0      0 0.0.0.0:9866            0.0.0.0:*               LISTEN      53826/java          
tcp6       0      0 :::111                  :::*                    LISTEN      1/systemd           
tcp6       0      0 :::22                   :::*                    LISTEN      865/sshd            
tcp6       0      0 ::1:631                 :::*                    LISTEN      862/cupsd           
tcp6       0      0 ::1:25                  :::*                    LISTEN      1159/master         
tcp6       0      0 ::1:6011  

æŸ¥çœ‹ç¨‹åºï¼š
æ‰§è¡Œjps
[root@hadoop102 ~]# jps
53826 DataNode
54306 JobHistoryServer
59973 Jps
53720 NameNode
54142 NodeManager
[root@hadoop102 ~]#
æŸ¥çœ‹53720 NameNode
æ‰§è¡Œï¼šnetstat -nltp | grep 53720
[root@hadoop102 ~]# netstat -nltp | grep 53720
tcp        0      0 0.0.0.0:9870            0.0.0.0:*               LISTEN      53720/java          
tcp        0      0 192.168.250.102:8020    0.0.0.0:*               LISTEN      53720/java          
[root@hadoop102 ~]#

å®‰è£…tree
æ‰§è¡Œï¼šyum provides tree
[root@hadoop102 ~]# yum provides tree
å®‰è£… iotop
æ‰§è¡Œï¼š[root@hadoop102 ~]# yum provides iotop
ç„¶åå†æ‰§è¡Œï¼š
[root@hadoop102 ~]# yum install -y iotop tree
ä¸éœ€è¦å®‰è£…ï¼Œå‰é¢å·²å®‰è£…

æ‰§è¡Œ
Tree ä»¥ğŸŒ²å½¢çš„ç»“æ„æŸ¥çœ‹ç›®å½•
æ‰§è¡Œï¼š[root@hadoop102 ~]# tree /opt/module/hadoop-3.1.3/etc/

[root@hadoop102 ~]# tree /opt/module/hadoop-3.1.3/etc/
/opt/module/hadoop-3.1.3/etc/
â””â”€â”€ hadoop
    â”œâ”€â”€ capacity-scheduler.xml
    â”œâ”€â”€ configuration.xsl
    â”œâ”€â”€ container-executor.cfg
    â”œâ”€â”€ core-site.xml
    â”œâ”€â”€ hadoop-env.cmd
    â”œâ”€â”€ hadoop-env.sh
    â”œâ”€â”€ hadoop-metrics2.properties
    â”œâ”€â”€ hadoop-policy.xml
    â”œâ”€â”€ hadoop-user-functions.sh.example
    â”œâ”€â”€ hdfs-site.xml
    â”œâ”€â”€ httpfs-env.sh
    â”œâ”€â”€ httpfs-log4j.properties
    â”œâ”€â”€ httpfs-signature.secret
    â”œâ”€â”€ httpfs-site.xml
    â”œâ”€â”€ kms-acls.xml
    â”œâ”€â”€ kms-env.sh
    â”œâ”€â”€ kms-log4j.properties
    â”œâ”€â”€ kms-site.xml
    â”œâ”€â”€ log4j.properties
    â”œâ”€â”€ mapred-env.cmd
    â”œâ”€â”€ mapred-env.sh
    â”œâ”€â”€ mapred-queues.xml.template
    â”œâ”€â”€ mapred-site.xml
    â”œâ”€â”€ shellprofile.d
    â”‚   â””â”€â”€ example.sh
    â”œâ”€â”€ ssl-client.xml.example
    â”œâ”€â”€ ssl-server.xml.example
    â”œâ”€â”€ user_ec_policies.xml.template
    â”œâ”€â”€ workers
    â”œâ”€â”€ yarn-env.cmd
    â”œâ”€â”€ yarn-env.sh
    â”œâ”€â”€ yarnservice-log4j.properties
    â””â”€â”€ yarn-site.xml

2 directories, 32 files
[root@hadoop102 ~]#



top æŸ¥çœ‹ç³»ç»Ÿè¿›ç¨‹
[root@hadoop102 ~]# top
top - 07:53:11 up  8:45,  3 users,  load average: 0.00, 0.01, 0.05
Tasks: 185 total,   1 running, 184 sleeping,   0 stopped,   0 zombie
%Cpu(s):  0.7 us,  0.7 sy,  0.0 ni, 98.6 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st
KiB Mem :  3865284 total,   437536 free,  1364036 used,  2063712 buff/cache
KiB Swap:  2097148 total,  2092112 free,     5036 used.  2120640 avail Mem

free -hæŸ¥çœ‹å†…å­˜
[root@hadoop102 ~]# free -h
              total        used        free      shared  buff/cache   available
Mem:           3.7G        1.3G        427M        7.6M        2.0G        2.0G
Swap:          2.0G        4.9M        2.0G
[root@hadoop102 ~]#

df -h æŸ¥çœ‹å­˜å‚¨
[root@hadoop102 ~]# df -h
æ–‡ä»¶ç³»ç»Ÿ        å®¹é‡  å·²ç”¨  å¯ç”¨ å·²ç”¨% æŒ‚è½½ç‚¹
/dev/sda3        18G  6.2G   12G   35% /
devtmpfs        1.9G     0  1.9G    0% /dev
tmpfs           1.9G     0  1.9G    0% /dev/shm
tmpfs           1.9G  9.3M  1.9G    1% /run
tmpfs           1.9G     0  1.9G    0% /sys/fs/cgroup
/dev/sda1       297M  157M  141M   53% /boot
tmpfs           378M  8.0K  378M    1% /run/user/42
tmpfs           378M     0  378M    0% /run/user/1001
tmpfs           378M   32K  378M    1% /run/user/0
/dev/sr0        4.3G  4.3G     0  100% /run/media/root/CentOS 7 x86_64




du æŸ¥çœ‹æ–‡ä»¶å¤§å°
Iotop æŸ¥çœ‹ç£ç›˜è¯»å†™æ€§èƒ½
[root@hadoop102 ~]# iotop

History
æŸ¥å†å²æ•²è¿‡çš„å‘½ä»¤
[root@hadoop102 ~]# history
1  vi /etc/sysconfig/network-scripts/ifcfg-ens33
    2  systemctl restart network
    3  ping baidu.com
    4  vim /etc/hostname
    5  vim /etc/hosts
    6  vi /etc/sysconfig/network-scripts/ifcfg-ens33
    7  systemctl restart network
    8  ping baidu.com
    9  sudo systemctl stop firewalls 
   10  systemctl stop firewalls
   11  reboot
   12  sudo systemctl stop firewalld
   13  sudo systemctl disable firewalld
   14  ssh hadoop101
   15  vim /etc/hosts
   16  ssh hadoop101
   17  sudo useradd chenbk
   18  sudo passwd chenbk
   19  vi sudo
   20  cd /etc/sudoers.d/
   21  vi sudo
   22  cd
   23  vi sudo
   24  visudo
   25  su - chenbk
   26  su - chenbk
   27  source /etc/profile
   28  hadoop version
   29  su - chenbk
   30  vim /etc/ntp.conf
   31  vim /etc/sysconfig/ntpd
   32  systemctl start ntpd
   33  systemctl enable ntpd
   34  systemctl status ntpd
   35  data
   36  date
   37  netstat -nltp
   38  jps
   39  netstat -nltp | grep 4380
   40  netstat -nltp | grep 53720
   41  yum provides tree
   42  yum provides iotop
   43  yum istall -y iotop tree
   44  yum install -y iotop tree
   45  tree /opt/module/hadoop-3.1.3/etc/
   46  top
   47  free -h
   48  df -h
   49  du /opt/module/hadoop-3.1.3/
   50  iotop
   51  history
3ï¼å¸¸ç”¨å‘½ä»¤å®æ“
[root@hadoop102 ~]# hadoop fs -df -h
2021-01-16 22:21:28,758 INFO Configuration.deprecation: No unit for dfs.client.datanode-restart.timeout(30) assuming SECONDS
Filesystem               Size  Used  Available  Use%
hdfs://hadoop102:8020  53.1 G  36 K     36.1 G    0%
[root@hadoop102 ~]#
ä»æœ¬åœ°ä¸Šä¼ æ–‡ä»¶åˆ°hdfs
æ‰§è¡Œï¼šhadoop fs -put LICENSE.txt /
[chenbk@hadoop102 hadoop-3.1.3]$ hadoop fs -put LICENSE.txt /
2021-01-16 22:27:27,862 INFO Configuration.deprecation: No unit for dfs.client.datanode-restart.timeout(30) assuming SECONDS
2021-01-16 22:27:28,484 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
[chenbk@hadoop102 hadoop-3.1.3]$
 
æŸ¥çœ‹å‘½ä»¤çš„å¸®å¿™ä¿¡æ¯ï¼ˆhelpï¼‰
[chenbk@hadoop102 hadoop-3.1.3]$ hadoop fs -help put
å‘½ä»¤copyToLocalå’ŒputåŸºæœ¬ä¸€è‡´
ï¼ˆ4ï¼‰-moveFromLocalï¼šä»æœ¬åœ°å‰ªåˆ‡ç²˜è´´åˆ°HDFS
ï¼ˆ5ï¼‰-appendToFileï¼šè¿½åŠ ä¸€ä¸ªæ–‡ä»¶åˆ°å·²ç»å­˜åœ¨çš„æ–‡ä»¶æœ«å°¾
[chenbk@hadoop102 hadoop-3.1.3]$ hadoop fs -appendToFile LICENSE.txt /README.txt
åœ¨hfdsä¸Šè¿½åŠ å­—æ®µ
æ‰§è¡Œï¼šhadoop fs -appendToFile - /LICENSE.txt
[chenbk@hadoop102 hadoop-3.1.3]$ hadoop fs -appendToFile - /LICENSE.txt
2021-01-16 22:42:32,780 INFO Configuration.deprecation: No unit for dfs.client.datanode-restart.timeout(30) assuming SECONDS
caddsc
chenbk
cehkbk
124434353^[^H^[[3~^[[3~
^C2021-01-16 22:43:24,936 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
æŒ‰ctrl+c é€€å‡º
 p-
ä»hdfsä¸‹è½½åˆ°æœ¬åœ°
æ‰§è¡Œï¼šhadoop fs -get /LICENSE.txt ./
[chenbk@hadoop102 hadoop-3.1.3]$ hadoop fs -get /LICENSE.txt ./
ï¼ˆ12ï¼‰-getï¼šç­‰åŒäºcopyToLocalï¼Œå°±æ˜¯ä»HDFSä¸‹è½½æ–‡ä»¶åˆ°æœ¬åœ°
[atguigu@hadoop102 hadoop-3.1.3]$ hadoop fs -get /sanguo/shuguo/kongming.txt ./

ï¼ˆ13ï¼‰-getmergeï¼šåˆå¹¶ä¸‹è½½å¤šä¸ªæ–‡ä»¶ï¼Œæ¯”å¦‚HDFSçš„ç›®å½• /user/atguigu/testä¸‹æœ‰å¤šä¸ªæ–‡ä»¶:log.1, log.2,log.3,...
[atguigu@hadoop102 hadoop-3.1.3]$ hadoop fs -getmerge /user/atguigu/test/* ./zaiyiqi.txt


package chenbk.hdfs;

import org.junit.Test;

import java.io.IOException;
import java.net.URI;
import java.nio.file.FileSystem;

public class HDFSClient {
    @Test
    public  void test() throws IOException,InterruptedException{
        //1ã€æ–°å»ºHDFSå¯¹è±¡
        FileSystem fileSystem=FileSystem.get(URI.create())
    }
}


package chenbk.hdfs;

import org.junit.Test;

import java.io.IOException;
import java.net.URI;
import java.nio.file.FileSystem;

public class HDFSClient {
    @Test
    public  void test() throws IOException,InterruptedException{
        //1ã€æ–°å»ºHDFSå¯¹è±¡
        FileSystem fileSystem=FileSystem.get(URI.create())
    }
}





åˆ é™¤æ–‡ä»¶ï¼š
Hadoop fs -rm -r /æ–‡ä»¶å
[chenbk@hadoop102 ~]$ hadoop fs -rm -r /tmp
2021-03-13 22:50:41,209 INFO Configuration.deprecation: No unit for dfs.client.datanode-restart.timeout(30) assuming SECONDS
Deleted /tmp
[chenbk@hadoop102 ~]$ hadoop fs -rm -r /OCå¼•å¯¼èƒŒæ™¯å›¾
2021-03-13 22:51:11,684 INFO Configuration.deprecation: No unit for dfs.client.datanode-restart.timeout(30) assuming SECONDS
Deleted /OCå¼•å¯¼èƒŒæ™¯å›¾
[chenbk@hadoop102 ~]$

æ‰©å±•æ•°æ®èŠ‚ç‚¹
1ã€ä¿®æ”¹IPåœ°å€å’Œä¸»æœºåç§°
2ã€ä¿®æ”¹IPåœ°å€å’Œä¸»æœºåç§°
3ã€æ‰§è¡Œrsync 
sudo rsync -av /opt/module hadoop101:/opt

[chenbk@hadoop102 ~]$ sudo rsync -av /opt/module hadoop101:/opt
root@hadoop101's password:
sending incremental file list
module/
module/hadoop-3.1.3/
module/hadoop-3.1.3/data/
module/hadoop-3.1.3/data/date/
module/hadoop-3.1.3/data/date/in_ã€ã€ã€ã€ã€ã€

å°±æ˜¯æŠŠ102çš„é…ç½®æ‹·è´åˆ°101ä¸­

4ã€æ‰§è¡Œåˆ é™¤æ—¥å¿—çš„å‘½ä»¤
rm -rf data logs edits.xml fsimage.xml
[chenbk@hadoop102 hadoop-3.1.3]$ rm -rf data logs edits.xml fsimage.xml
[chenbk@hadoop102 hadoop-3.1.3]$ ll
5ã€æŠŠç¯å¢ƒå˜é‡æ‹·è´åˆ°105ä¸­
sudo rsync -av /etc/profile.d hadoop101:/etc

[chenbk@hadoop102 hadoop-3.1.3]$ sudo rsync -av /etc/profile.d hadoop101:/etc
[sudo] chenbk çš„å¯†ç ï¼š
root@hadoop101's password:
sending incremental file list
profile.d/
profile.d/my_env.sh
sent 667 bytes  received 45 bytes  94.93 bytes/sec
total size is 16,607  speedup is 23.32
[chenbk@hadoop102 hadoop-3.1.3]$ jps
6ã€åœ¨101ä¸­æŸ¥çœ‹hadoopæƒ…å†µ
[chenbk@hadoop101 ~]$ hadoop version
Hadoop 3.1.3
Source code repository https://gitbox.apache.org/repos/asf/hadoop.git -r ba631c436b806728f8ec2f54ab1e289526c90579
Compiled by ztang on 2019-09-12T02:47Z
Compiled with protoc 2.5.0
From source with checksum ec785077c385118ac91aadde5ec9799
This command was run using /opt/module/hadoop-3.1.3/share/hadoop/common/hadoop-common-3.1.3.jar
7ã€å¯åŠ¨101é›†ç¾¤
[chenbk@hadoop101 ~]$ hdfs --daemon start datanode
[chenbk@hadoop101 ~]$ jps
9431 Jps
9375 DataNode
8ã€å†æ‰§è¡Œå¯åŠ¨å‘½ä»¤
[chenbk@hadoop101 ~]$ yarn --daemon start nodemanager
[chenbk@hadoop101 ~]$ jps
9573 Jps
9511 NodeManager
9375 DataNode
[chenbk@hadoop101 ~]$






Hive
å®‰è£…
å…ˆä¸Šä¼ æ–‡ä»¶è‡³[chenbk@hadoop102 software]$ ll

[chenbk@hadoop102 software]$ ll
æ€»ç”¨é‡ 1340000
-rw-rw-r--. 1 chenbk chenbk    318972 4æœˆ  30 11:09 01_mysql-community-common-5.7.29-1.el7.x86_64.rpm
-rw-rw-r--. 1 chenbk chenbk   2596180 4æœˆ  30 11:09 02_mysql-community-libs-5.7.29-1.el7.x86_64.rpm
-rw-rw-r--. 1 chenbk chenbk   1353080 4æœˆ  30 11:09 03_mysql-community-libs-compat-5.7.29-1.el7.x86_64.rpm
-rw-rw-r--. 1 chenbk chenbk  27768112 4æœˆ  30 11:09 04_mysql-community-client-5.7.29-1.el7.x86_64.rpm
-rw-rw-r--. 1 chenbk chenbk 183618644 4æœˆ  30 11:10 05_mysql-community-server-5.7.29-1.el7.x86_64.rpm
-rw-rw-r--. 1 chenbk chenbk 278813748 4æœˆ  30 11:10 apache-hive-3.1.2-bin.tar.gz
-rw-rw-r--. 1 chenbk chenbk  62945274 4æœˆ  30 11:10 apache-tez-0.9.2-bin.tar.gz
-rw-rw-r--. 1 chenbk chenbk   9311744 4æœˆ  30 11:10 apache-zookeeper-3.5.7-bin.tar.gz
-rw-rw-r--. 1 chenbk chenbk       396 4æœˆ  30 11:10 chenbk.sh
-rw-rw-r--. 1 chenbk chenbk 338075860 4æœˆ  30 11:25 hadoop-3.1.3.tar.gz
-rw-rw-r--. 1 chenbk chenbk 195013152 4æœˆ  30 11:25 jdk-8u212-linux-x64.tar.gz
-rw-rw-r--. 1 chenbk chenbk  70159813 4æœˆ  30 11:10 kafka_2.11-2.4.1.tgz
-rw-rw-r--. 1 chenbk chenbk   1006956 4æœˆ  30 11:10 mysql-connector-java-5.1.48.jar
-rw-rw-r--. 1 chenbk chenbk       256 4æœˆ  30 11:10 remove_mysql.sh
-rw-rw-r--. 1 chenbk chenbk 201142612 4æœˆ  30 11:10 spark-2.1.1-bin-hadoop2.7.tgz
-rw-rw-r--. 1 chenbk chenbk       482 4æœˆ  30 11:11 xsync
[chenbk@hadoop102 software]$

å¸è½½è‡ªå¸¦çš„Mysql-libsï¼ˆå¦‚æœä¹‹å‰å®‰è£…è¿‡mysqlï¼Œè¦å…¨éƒ½å¸è½½æ‰ï¼‰
å…ˆæŸ¥çœ‹
[chenbk@hadoop102 software]$ rpm -qa | grep -i -E mysql\|mariadb
mariadb-libs-5.5.68-1.el7.x86_64

å†æ‰§è¡Œ
[chenbk@hadoop102 software]$ rpm -qa | grep -i -E mysql\|mariadb | xargs -n1 sudo rpm -e --nodeps
è¿›è¡Œå¸è½½

[chenbk@hadoop102 software]$ rpm -qa | grep -i -E mysql\|mariadb | xargs -n1 sudo rpm -e --nodeps
[sudo] chenbk çš„å¯†ç ï¼š
[chenbk@hadoop102 software]$ rpm -qa | grep -i -E mysql\|mariadb
[chenbk@hadoop102 software]$

å¦‚æœæ²¡æœ‰æ‰¾åˆ°å°±ä¸€è¡Œï¼ˆä¸´æ—¶å¯†ç ï¼‰ï¼Œéœ€è¦æ‰§è¡Œæˆ–è€…ç›´æ¥æ‰§è¡Œï¼šsudo rm -rf /var/lib/mysql
[chenbk@hadoop102 software]$ sudo rm -rf /var/lib/mysql

å®‰è£…5ä¸ªåŒ…
[chenbk@hadoop102 software]$ ls *.rpm | xargs -n1 sudo rpm -ivh


è­¦å‘Šï¼š01_mysql-community-common-5.7.29-1.el7.x86_64.rpm: å¤´V3 DSA/SHA1 Signature, å¯†é’¥ ID 5072e1f5: NOKEY
å‡†å¤‡ä¸­...                          ################################# [100%]
æ­£åœ¨å‡çº§/å®‰è£…...
   1:mysql-community-common-5.7.29-1.e################################# [100%]
è­¦å‘Šï¼š02_mysql-community-libs-5.7.29-1.el7.x86_64.rpm: å¤´V3 DSA/SHA1 Signature, å¯†é’¥ ID 5072e1f5: NOKEY
å‡†å¤‡ä¸­...                          ################################# [100%]
æ­£åœ¨å‡çº§/å®‰è£…...
   1:mysql-community-libs-5.7.29-1.el7################################# [100%]
è­¦å‘Šï¼š03_mysql-community-libs-compat-5.7.29-1.el7.x86_64.rpm: å¤´V3 DSA/SHA1 Signature, å¯†é’¥ ID 5072e1f5: NOKEY
å‡†å¤‡ä¸­...                          ################################# [100%]
æ­£åœ¨å‡çº§/å®‰è£…...
   1:mysql-community-libs-compat-5.7.2################################# [100%]
è­¦å‘Šï¼š04_mysql-community-client-5.7.29-1.el7.x86_64.rpm: å¤´V3 DSA/SHA1 Signature, å¯†é’¥ ID 5072e1f5: NOKEY
å‡†å¤‡ä¸­...                          ################################# [100%]
æ­£åœ¨å‡çº§/å®‰è£…...
   1:mysql-community-client-5.7.29-1.e################################# [100%]
è­¦å‘Šï¼š05_mysql-community-server-5.7.29-1.el7.x86_64.rpm: å¤´V3 DSA/SHA1 Signature, å¯†é’¥ ID 5072e1f5: NOKEY
å‡†å¤‡ä¸­...                          ################################# [100%]
æ­£åœ¨å‡çº§/å®‰è£…...
   1:mysql-community-server-5.7.29-1.e################################# [100%]
[chenbk@hadoop102 software]$


å¯åŠ¨mysql
[chenbk@hadoop102 software]$ sudo systemctl start mysqld
[chenbk@hadoop102 software]$

æŸ¥çœ‹éšæœºå¯†ç 
[chenbk@hadoop102 software]$ sudo cat /var/log/mysqld.log | grep password
[chenbk@hadoop102 software]$ sudo cat /var/log/mysqld.log | grep password
2021-04-30T16:17:58.946751Z 1 [Note] A temporary password is generated for root@localhost: QHjXsiWa0i.n
2021-04-30T16:18:42.041883Z 2 [Note] Access denied for user 'root'@'localhost' (using password: YES)
2021-04-30T16:21:06.417374Z 3 [Note] Access denied for user 'root'@'localhost' (using password: YES)
2021-04-30T16:21:35.075057Z 4 [Note] Access denied for user 'root'@'localhost' (using password: YES)
2021-04-30T16:21:40.213499Z 5 [Note] Access denied for user 'root'@'localhost' (using password: YES)
2021-04-30T16:52:06.664195Z 0 [Note] Shutting down plugin 'validate_password'
2021-04-30T16:52:10.538565Z 0 [Note] Shutting down plugin 'sha256_password'
2021-04-30T16:52:10.538582Z 0 [Note] Shutting down plugin 'mysql_native_password'
2021-05-01T01:04:06.967330Z 0 [Note] Shutting down plugin 'validate_password'
2021-05-01T01:04:08.743080Z 0 [Note] Shutting down plugin 'sha256_password'
2021-05-01T01:04:08.743086Z 0 [Note] Shutting down plugin 'mysql_native_password'
2021-05-01T01:07:16.250258Z 0 [Note] Shutting down plugin 'validate_password'
2021-05-01T01:07:18.029547Z 0 [Note] Shutting down plugin 'sha256_password'
2021-05-01T01:07:18.029553Z 0 [Note] Shutting down plugin 'mysql_native_password'
2021-05-01T01:20:14.907577Z 0 [Note] Shutting down plugin 'validate_password'
2021-05-01T01:20:16.403600Z 0 [Note] Shutting down plugin 'sha256_password'
2021-05-01T01:20:16.403606Z 0 [Note] Shutting down plugin 'mysql_native_password'
2021-05-01T01:23:23.773211Z 0 [Note] Shutting down plugin 'validate_password'
2021-05-01T01:23:25.705880Z 0 [Note] Shutting down plugin 'sha256_password'
2021-05-01T01:23:25.705887Z 0 [Note] Shutting down plugin 'mysql_native_password'
2021-05-01T01:44:57.084436Z 0 [Note] Shutting down plugin 'validate_password'
2021-05-01T01:44:58.313913Z 0 [Note] Shutting down plugin 'sha256_password'
2021-05-01T01:44:58.313928Z 0 [Note] Shutting down plugin 'mysql_native_password'
2021-05-01T01:51:00.807486Z 2 [Note] Access denied for user 'root'@'localhost' (using password: YES)
2021-05-01T01:52:22.478113Z 5 [Note] Access denied for user 'root'@'localhost' (using password: YES)
2021-05-01T01:52:48.428016Z 6 [Note] Access denied for user 'root'@'localhost' (using password: YES)
2021-05-01T01:53:06.758106Z 7 [Note] Access denied for user 'root'@'localhost' (using password: NO)
2021-05-01T01:53:44.722286Z 8 [Note] Access denied for user 'root'@'localhost' (using password: NO)
2021-05-01T01:53:59.593268Z 9 [Note] Access denied for user 'root'@'localhost' (using password: NO)
2021-05-01T01:56:02.623383Z 11 [Note] Access denied for user 'root'@'localhost' (using password: YES)
2021-05-01T01:57:57.677815Z 12 [Note] Access denied for user 'root'@'localhost' (using password: YES)
2021-05-01T01:58:59.980612Z 13 [Note] Access denied for user 'root'@'localhost' (using password: YES)
2021-05-01T01:59:33.127990Z 14 [Note] Access denied for user 'root'@'localhost' (using password: YES)
2021-05-01T02:00:22.767703Z 0 [Note] Shutting down plugin 'validate_password'
2021-05-01T02:00:24.196724Z 0 [Note] Shutting down plugin 'sha256_password'
2021-05-01T02:00:24.196732Z 0 [Note] Shutting down plugin 'mysql_native_password'
2021-05-01T02:05:10.546555Z 0 [Warning] Couldn't load plugin named 'validate_password' with soname 'validate_password.so'.
2021-05-01T02:06:57.767835Z 0 [Note] Shutting down plugin 'sha256_password'
2021-05-01T02:06:57.767841Z 0 [Note] Shutting down plugin 'mysql_native_password'
2021-05-01T02:09:55.721847Z 1 [Note] A temporary password is generated for root@localhost: )y%CXj:QO1kC
[chenbk@hadoop102 software]$ sudo cat /var/log/mysqld.log | grep password
2021-04-30T16:17:58.946751Z 1 [Note] A temporary password is generated for root@localhost: QHjXsiWa0i.n
2021-04-30T16:18:42.041883Z 2 [Note] Access denied for user 'root'@'localhost' (using password: YES)
2021-04-30T16:21:06.417374Z 3 [Note] Access denied for user 'root'@'localhost' (using password: YES)
2021-04-30T16:21:35.075057Z 4 [Note] Access denied for user 'root'@'localhost' (using password: YES)
2021-04-30T16:21:40.213499Z 5 [Note] Access denied for user 'root'@'localhost' (using password: YES)
2021-04-30T16:52:06.664195Z 0 [Note] Shutting down plugin 'validate_password'
2021-04-30T16:52:10.538565Z 0 [Note] Shutting down plugin 'sha256_password'
2021-04-30T16:52:10.538582Z 0 [Note] Shutting down plugin 'mysql_native_password'
2021-05-01T01:04:06.967330Z 0 [Note] Shutting down plugin 'validate_password'
2021-05-01T01:04:08.743080Z 0 [Note] Shutting down plugin 'sha256_password'
2021-05-01T01:04:08.743086Z 0 [Note] Shutting down plugin 'mysql_native_password'
2021-05-01T01:07:16.250258Z 0 [Note] Shutting down plugin 'validate_password'
2021-05-01T01:07:18.029547Z 0 [Note] Shutting down plugin 'sha256_password'
2021-05-01T01:07:18.029553Z 0 [Note] Shutting down plugin 'mysql_native_password'
2021-05-01T01:20:14.907577Z 0 [Note] Shutting down plugin 'validate_password'
2021-05-01T01:20:16.403600Z 0 [Note] Shutting down plugin 'sha256_password'
2021-05-01T01:20:16.403606Z 0 [Note] Shutting down plugin 'mysql_native_password'
2021-05-01T01:23:23.773211Z 0 [Note] Shutting down plugin 'validate_password'
2021-05-01T01:23:25.705880Z 0 [Note] Shutting down plugin 'sha256_password'
2021-05-01T01:23:25.705887Z 0 [Note] Shutting down plugin 'mysql_native_password'
2021-05-01T01:44:57.084436Z 0 [Note] Shutting down plugin 'validate_password'
2021-05-01T01:44:58.313913Z 0 [Note] Shutting down plugin 'sha256_password'
2021-05-01T01:44:58.313928Z 0 [Note] Shutting down plugin 'mysql_native_password'
2021-05-01T01:51:00.807486Z 2 [Note] Access denied for user 'root'@'localhost' (using password: YES)
2021-05-01T01:52:22.478113Z 5 [Note] Access denied for user 'root'@'localhost' (using password: YES)
2021-05-01T01:52:48.428016Z 6 [Note] Access denied for user 'root'@'localhost' (using password: YES)
2021-05-01T01:53:06.758106Z 7 [Note] Access denied for user 'root'@'localhost' (using password: NO)
2021-05-01T01:53:44.722286Z 8 [Note] Access denied for user 'root'@'localhost' (using password: NO)
2021-05-01T01:53:59.593268Z 9 [Note] Access denied for user 'root'@'localhost' (using password: NO)
2021-05-01T01:56:02.623383Z 11 [Note] Access denied for user 'root'@'localhost' (using password: YES)
2021-05-01T01:57:57.677815Z 12 [Note] Access denied for user 'root'@'localhost' (using password: YES)
2021-05-01T01:58:59.980612Z 13 [Note] Access denied for user 'root'@'localhost' (using password: YES)
2021-05-01T01:59:33.127990Z 14 [Note] Access denied for user 'root'@'localhost' (using password: YES)
2021-05-01T02:00:22.767703Z 0 [Note] Shutting down plugin 'validate_password'
2021-05-01T02:00:24.196724Z 0 [Note] Shutting down plugin 'sha256_password'
2021-05-01T02:00:24.196732Z 0 [Note] Shutting down plugin 'mysql_native_password'
2021-05-01T02:05:10.546555Z 0 [Warning] Couldn't load plugin named 'validate_password' with soname 'validate_password.so'.
2021-05-01T02:06:57.767835Z 0 [Note] Shutting down plugin 'sha256_password'
2021-05-01T02:06:57.767841Z 0 [Note] Shutting down plugin 'mysql_native_password'

2021-05-01T02:09:55.721847Z 1 [Note] A temporary password is generated for root@localhost: )y%CXj:QO1kC
è¿›è¡Œç™»å½•
æ³¨æ„æ˜¯ç™»å½•çš„æ—¥å¿—ï¼Œä¸´æ—¶å¯†ç æ˜¯2021-05-01T02:09:55.721847Z 1 [Note] A temporary password is generated for root@localhost: )y%CXj:QO1kC
ä¸­çš„ï¼šy%CXj:QO1kC

ç™»å½•
[chenbk@hadoop102 software]$ mysql -uroot -p')y%CXj:QO1kC'
mysql: [Warning] Using a password on the command line interface can be insecure.
Welcome to the MySQL monitor.  Commands end with ; or \g.
Your MySQL connection id is 3
Server version: 5.7.29

Copyright (c) 2000, 2020, Oracle and/or its affiliates. All rights reserved.

Oracle is a registered trademark of Oracle Corporation and/or its
affiliates. Other names may be trademarks of their respective
owners.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

mysql>

mysqlä¸å…è®¸è®¾ç½®ç®€å•å¯†ç ï¼Œå¯ä»¥æ‰§è¡Œä»¥ä¸‹æ­¥éª¤

å…ˆè®¾ç½®å¤æ‚çš„å¯†ç 
mysql> set password=password("Qs23=zs32");
Query OK, 0 rows affected, 1 warning (0.00 sec)

å†æ›´æ”¹mysqlå¯†ç ç­–ç•¥
mysql> set global validate_password_length=4;
Query OK, 0 rows affected (0.00 sec)

mysql> set global validate_password_policy=0;
Query OK, 0 rows affected (0.00 sec)

è®¾ç½®ç®€å•å¥½è®°çš„å¯†ç 
mysql> set password=password("111111");
Query OK, 0 rows affected, 1 warning (0.00 sec)

è¿›å…¥msyqlåº“
mysql> use mysql
Reading table information for completion of table and column names
You can turn off this feature to get a quicker startup with -A

Database changed

æŸ¥è¯¢userè¡¨
mysql> select user, host from user;
+---------------+-----------+
| user          | host      |
+---------------+-----------+
| mysql.session | localhost |
| mysql.sys     | localhost |
| root          | localhost |
+---------------+-----------+
3 rows in set (0.00 sec)

ä¿®æ”¹userè¡¨ï¼ŒæŠŠHostè¡¨å†…å®¹ä¿®æ”¹ä¸º%
mysql> update user set host="%" where user="root";
Query OK, 1 row affected (0.00 sec)
Rows matched: 1  Changed: 1  Warnings: 0

mysql> select user, host from user;
+---------------+-----------+
| user          | host      |
+---------------+-----------+
| root          | %         |
| mysql.session | localhost |
| mysql.sys     | localhost |
+---------------+-----------+
3 rows in set (0.00 sec)

åˆ·æ–°æƒé™
mysql> flush privileges;
Query OK, 0 rows affected (0.00 sec)

é€€å‡º
mysql> quit;
Bye

mysqlå®‰è£…å®Œæˆ

hiveå®‰è£…
å…ˆåˆ é™¤ä¹‹å‰å®‰è£…è¿‡çš„ç›®å½•åŠæ–‡ä»¶
[chenbk@hadoop102 module]$ ll
æ€»ç”¨é‡ 4
drwxr-xr-x. 11 chenbk chenbk  173 4æœˆ  30 12:05 hadoop-3.1.3
drwxrwxr-x. 11 chenbk chenbk  213 4æœˆ  30 12:36 hive
drwxr-xr-x.  7 chenbk chenbk  245 4æœˆ   2 2019 jdk1.8.0_212
drwxrwxr-x.  7 chenbk chenbk  101 4æœˆ  30 12:37 kafka
drwxrwxr-x.  5 chenbk chenbk 4096 4æœˆ  30 12:36 tez
drwxrwxr-x.  8 chenbk chenbk  160 4æœˆ  30 12:38 zookeeper

åˆ é™¤
[chenbk@hadoop102 module]$ rm -rf hive/
[chenbk@hadoop102 module]$ rm -rf tez/

[chenbk@hadoop102 module]$ ll
æ€»ç”¨é‡ 0
drwxr-xr-x. 11 chenbk chenbk 173 4æœˆ  30 12:05 hadoop-3.1.3
drwxr-xr-x.  7 chenbk chenbk 245 4æœˆ   2 2019 jdk1.8.0_212
drwxrwxr-x.  7 chenbk chenbk 101 4æœˆ  30 12:37 kafka
drwxrwxr-x.  8 chenbk chenbk 160 4æœˆ  30 12:38 zookeeper


å…ˆä¿®æ”¹ç¯å¢ƒå˜é‡
[chenbk@hadoop102 software]$ sudo vim /etc/profile.d/my_env.sh

æ’å…¥ä»¥ä¸‹è¯­å¥(ä¹‹å‰å·²ç»å…¨éƒ¨é…ç½®å®Œæˆ)
#JAVA_HOME
export JAVA_HOME=/opt/module/jdk1.8.0_212
export PATH=$PATH:$JAVA_HOME/bin
##HADOOP_HOME
export HADOOP_HOME=/opt/module/hadoop-3.1.3
export PATH=$PATH:$HADOOP_HOME/bin
export PATH=$PATH:$HADOOP_HOME/sbin
#HIVE_HOME
export HIVE_HOME=/opt/module/hive
export PATH=$PATH:$HIVE_HOME/bin
#FLUME_HOME
export FLUME_HOME=/opt/module/flume
export PATH=$PATH:$FLUME_HOME/bin

#KAFKA_HOME
export KAFKA_HOME=/opt/module/kafka
export PATH=$PATH:$KAFKA_HOME/bin

#ZOOKEEPER_HOME
export ZOOKEEPER_HOME=/opt/module/zookeeper
export PATH=$PATH:$ZOOKEEPER_HOME/bin

åˆ›å»ºHive.sh æ‰§è¡Œè„šæœ¬
[chenbk@hadoop102 software]$ vim Hive.sh
æ’å…¥ä»¥ä¸‹è¯­å¥
#!/bin/bash
HADOOP_HOME=/opt/module/hadoop-3.1.3
HIVE_HOME=/opt/module/hive
tar -zxvf /opt/software/apache-hive-3.1.2-bin.tar.gz -C /opt/module/
mv /opt/module/apache-hive-3.1.2-bin /opt/module/hive
cp $HADOOP_HOME/share/hadoop/common/lib/guava-27.0-jre.jar $HIVE_HOME/lib/
rm $HIVE_HOME/lib/guava-19.0.jar
mv $HIVE_HOME/lib/log4j-slf4j-impl-2.10.0.jar $HIVE_HOME/lib/log4j-slf4j-impl-2.10.0.bak

æ‰§è¡Œ[chenbk@hadoop102 software]$ sh Hive.sh
apache-hive-3.1.2-bin/lib/accumulo-trace-1.7.3.jar
apache-hive-3.1.2-bin/lib/hive-llap-ext-client-3.1.2.jar
apache-hive-3.1.2-bin/lib/hive-hplsql-3.1.2.jar
apache-hive-3.1.2-bin/lib/antlr4-runtime-4.5.jar
apache-hive-3.1.2-bin/lib/org.abego.treelayout.core-1.0.1.jar
apache-hive-3.1.2-bin/lib/hive-streaming-3.1.2.jar
apache-hive-3.1.2-bin/lib/hive-kryo-registrator-3.1.2.jar
apache-hive-3.1.2-bin/jdbc/hive-jdbc-3.1.2-standalone.jar
apache-hive-3.1.2-bin/lib/hive-hcatalog-core-3.1.2.jar
apache-hive-3.1.2-bin/lib/hive-hcatalog-server-extensions-3.1.2.jar
apache-hive-3.1.2-bin/hcatalog/share/hcatalog/hive-hcatalog-streaming-3.1.2.jar
apache-hive-3.1.2-bin/hcatalog/share/hcatalog/hive-hcatalog-core-3.1.2.jar
apache-hive-3.1.2-bin/hcatalog/share/hcatalog/hive-hcatalog-pig-adapter-3.1.2.jar
apache-hive-3.1.2-bin/hcatalog/share/hcatalog/hive-hcatalog-server-extensions-3.1.2.jar
apache-hive-3.1.2-bin/hcatalog/share/webhcat/svr/lib/jersey-json-1.19.jar
apache-hive-3.1.2-bin/hcatalog/share/webhcat/svr/lib/jaxb-impl-2.2.3-1.jar
apache-hive-3.1.2-bin/hcatalog/share/webhcat/svr/lib/jackson-jaxrs-1.9.2.jar
apache-hive-3.1.2-bin/hcatalog/share/webhcat/svr/lib/jackson-xc-1.9.2.jar
apache-hive-3.1.2-bin/hcatalog/share/webhcat/svr/lib/jersey-core-1.19.jar
apache-hive-3.1.2-bin/hcatalog/share/webhcat/svr/lib/jsr311-api-1.1.1.jar
apache-hive-3.1.2-bin/hcatalog/share/webhcat/svr/lib/jersey-servlet-1.19.jar
apache-hive-3.1.2-bin/hcatalog/share/webhcat/svr/lib/hive-webhcat-3.1.2.jar
apache-hive-3.1.2-bin/hcatalog/share/webhcat/svr/lib/wadl-resourcedoc-doclet-1.4.jar
apache-hive-3.1.2-bin/hcatalog/share/webhcat/svr/lib/xercesImpl-2.9.1.jar
apache-hive-3.1.2-bin/hcatalog/share/webhcat/svr/lib/xml-apis-1.3.04.jar
apache-hive-3.1.2-bin/hcatalog/share/webhcat/svr/lib/commons-exec-1.1.jar
apache-hive-3.1.2-bin/hcatalog/share/webhcat/svr/lib/jul-to-slf4j-1.7.10.jar
apache-hive-3.1.2-bin/hcatalog/share/webhcat/java-client/hive-webhcat-java-client-3.1.2.jar

æŸ¥çœ‹å®‰è£…æƒ…å†µ
[chenbk@hadoop102 software]$ cd /opt/module/
[chenbk@hadoop102 module]$ ll
æ€»ç”¨é‡ 0
drwxr-xr-x. 11 chenbk chenbk 173 4æœˆ  30 12:05 hadoop-3.1.3
drwxrwxr-x. 10 chenbk chenbk 184 4æœˆ  30 22:42 hive
drwxr-xr-x.  7 chenbk chenbk 245 4æœˆ   2 2019 jdk1.8.0_212
drwxrwxr-x.  7 chenbk chenbk 101 4æœˆ  30 12:37 kafka
drwxrwxr-x.  8 chenbk chenbk 160 4æœˆ  30 12:38 zookeeper

Hiveå·²å®‰è£…å®Œæˆï¼Œä½†è¿˜éœ€è¦åæŠŠhiveå…ƒæ•°æ®é…ç½®åˆ°mysqlä¸­

å°†MySQLçš„JDBCé©±åŠ¨æ‹·è´åˆ°Hiveçš„libç›®å½•ä¸‹
cp /opt/software/mysql-connector-java-5.1.48.jar $HIVE_HOME/lib

[chenbk@hadoop102 software]$ cp /opt/software/mysql-connector-java-5.1.48.jar $HIVE_HOME/lib
[chenbk@hadoop102 software]$

åœ¨$HIVE_HOME/confç›®å½•ä¸‹æ–°å»ºhive-site.xmlæ–‡ä»¶
vim $HIVE_HOME/conf/hive-site.xml

åœ¨æ–‡ä»¶ä¸­æ·»åŠ ä»¥ä¸‹å†…å®¹

<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
    <property>
        <name>javax.jdo.option.ConnectionURL</name>
        <value>jdbc:mysql://hadoop102:3306/metastore?useSSL=false</value>
    </property>

    <property>
        <name>javax.jdo.option.ConnectionDriverName</name>
        <value>com.mysql.jdbc.Driver</value>
    </property>

    <property>
        <name>javax.jdo.option.ConnectionUserName</name>
        <value>root</value>
    </property>

    <property>
        <name>javax.jdo.option.ConnectionPassword</name>
        <value>111111</value>
    </property>

    <property>
        <name>hive.metastore.warehouse.dir</name>
        <value>/user/hive/warehouse</value>
    </property>

    <property>
        <name>hive.metastore.schema.verification</name>
        <value>false</value>
    </property>

    <property>
        <name>datanucleus.schema.autoCreateAll</name>
        <value>true</value> 
    </property>

    <property>
        <name>hive.metastore.uris</name>
        <value>thrift://hadoop102:9083</value>
    </property>

    <property>
    <name>hive.server2.thrift.port</name>
    <value>10000</value>
    </property>

    <property>
        <name>hive.server2.thrift.bind.host</name>
        <value>hadoop102</value>
    </property>

    <property>
        <name>hive.metastore.event.db.notification.api.auth</name>
        <value>false</value>
    </property>
<property>
    <name>hive.execution.engine</name>
    <value>tez</value>
</property>


</configuration>


å®‰è£…Tezå¼•æ“
å°†tezå®‰è£…åŒ…æ‹·è´åˆ°é›†ç¾¤
[chenbk@hadoop102 software]$ ll
\æ€»ç”¨é‡ 1340004
-rw-rw-r--. 1 chenbk chenbk    318972 4æœˆ  30 11:09 01_mysql-community-common-5.7.29-1.el7.x86_64.rpm
-rw-rw-r--. 1 chenbk chenbk   2596180 4æœˆ  30 11:09 02_mysql-community-libs-5.7.29-1.el7.x86_64.rpm
-rw-rw-r--. 1 chenbk chenbk   1353080 4æœˆ  30 11:09 03_mysql-community-libs-compat-5.7.29-1.el7.x86_64.rpm
-rw-rw-r--. 1 chenbk chenbk  27768112 4æœˆ  30 11:09 04_mysql-community-client-5.7.29-1.el7.x86_64.rpm
-rw-rw-r--. 1 chenbk chenbk 183618644 4æœˆ  30 11:10 05_mysql-community-server-5.7.29-1.el7.x86_64.rpm
-rw-rw-r--. 1 chenbk chenbk 278813748 4æœˆ  30 11:10 apache-hive-3.1.2-bin.tar.gz
-rw-rw-r--. 1 chenbk chenbk  62945274 4æœˆ  30 11:10 apache-tez-0.9.2-bin.tar.gz
-rw-rw-r--. 1 chenbk chenbk   9311744 4æœˆ  30 11:10 apache-zookeeper-3.5.7-bin.tar.gz
-rw-rw-r--. 1 chenbk chenbk       396 4æœˆ  30 11:10 chenbk.sh
-rw-rw-r--. 1 chenbk chenbk 338075860 4æœˆ  30 11:25 hadoop-3.1.3.tar.gz
-rw-rw-r--. 1 chenbk chenbk       396 4æœˆ  30 22:42 Hive.sh
-rw-rw-r--. 1 chenbk chenbk 195013152 4æœˆ  30 11:25 jdk-8u212-linux-x64.tar.gz
-rw-rw-r--. 1 chenbk chenbk  70159813 4æœˆ  30 11:10 kafka_2.11-2.4.1.tgz
-rw-rw-r--. 1 chenbk chenbk   1006956 4æœˆ  30 11:10 mysql-connector-java-5.1.48.jar
-rw-rw-r--. 1 chenbk chenbk       256 4æœˆ  30 11:10 remove_mysql.sh
-rw-rw-r--. 1 chenbk chenbk 201142612 4æœˆ  30 11:10 spark-2.1.1-bin-hadoop2.7.tgz
-rw-rw-r--. 1 chenbk chenbk       482 4æœˆ  30 11:11 xsync


è§£å‹taråŒ…
tar -zxvf /opt/software/apache-tez-0.9.2-bin.tar.gz -C /opt/module
[chenbk@hadoop102 software]$ tar -zxvf /opt/software/apache-tez-0.9.2-bin.tar.gz -C /opt/module

æŸ¥çœ‹è§£å‹æƒ…å†µ
[chenbk@hadoop102 module]$ ll
æ€»ç”¨é‡ 4
drwxr-xr-x.  5 chenbk chenbk 4096 3æœˆ  19 2019 apache-tez-0.9.2-bin
drwxr-xr-x. 11 chenbk chenbk  173 4æœˆ  30 12:05 hadoop-3.1.3
drwxrwxr-x. 10 chenbk chenbk  184 4æœˆ  30 22:42 hive
drwxr-xr-x.  7 chenbk chenbk  245 4æœˆ   2 2019 jdk1.8.0_212
drwxrwxr-x.  7 chenbk chenbk  101 4æœˆ  30 12:37 kafka
drwxrwxr-x.  8 chenbk chenbk  160 4æœˆ  30 12:38 zookeeper

é‡å‘½å
[chenbk@hadoop102 module]$ mv /opt/module/apache-tez-0.9.2-bin /opt/module/tez
[chenbk@hadoop102 module]$ ll
æ€»ç”¨é‡ 4
drwxr-xr-x. 11 chenbk chenbk  173 4æœˆ  30 12:05 hadoop-3.1.3
drwxrwxr-x. 10 chenbk chenbk  184 4æœˆ  30 22:42 hive
drwxr-xr-x.  7 chenbk chenbk  245 4æœˆ   2 2019 jdk1.8.0_212
drwxrwxr-x.  7 chenbk chenbk  101 4æœˆ  30 12:37 kafka
drwxr-xr-x.  5 chenbk chenbk 4096 3æœˆ  19 2019 tez
drwxrwxr-x.  8 chenbk chenbk  160 4æœˆ  30 12:38 zookeeper
[chenbk@hadoop102 module]$

ä¸Šä¼ tezä¾èµ–åˆ°HDFS
å…ˆå¯åŠ¨é›†ç¾¤
[chenbk@hadoop102 hadoop-3.1.3]$ sbin/start-dfs.sh
Starting namenodes on [hadoop102]
Starting datanodes
Starting secondary namenodes [hadoop104]
[chenbk@hadoop102 hadoop-3.1.3]$

[chenbk@hadoop103 hadoop-3.1.3]$ sbin/start-yarn.sh
Starting resourcemanager
Starting nodemanagers

æŸ¥çœ‹çŠ¶æ€
[chenbk@hadoop102 ~]$ sh jpsall
=====  hadoop102   =====
2453 NameNode
2582 DataNode
2925 NodeManager
=====  hadoop103   =====
1620 NodeManager
1335 DataNode
1503 ResourceManager
=====  hadoop104   =====
1778 NodeManager
1573 DataNode
1642 SecondaryNameNode
[chenbk@hadoop102 ~]$

ä¸Šä¼ tezä¾èµ–åˆ°HDFS
[chenbk@hadoop102 module]$ hadoop fs -mkdir /tez
2021-04-30 23:04:24,401 INFO Configuration.deprecation: No unit for dfs.client.datanode-restart.timeout(30) assuming SECONDS
mkdir: `/tez': File exists

å‘ç°å·²ç»å­˜åœ¨ï¼Œå°±å…ˆåˆ é™¤å…ˆï¼ˆåœ¨è¿™åˆ é™¤æ‰€æœ‰æ–‡ä»¶ï¼‰ï¼Œæ‰§è¡Œå‘½ä»¤
[chenbk@hadoop102 module]$ hadoop fs -rm -f hdfs://192.168.1.112:8020/*

å‘ç°æ˜¯ä¸ªç›®å½•
[chenbk@hadoop102 module]$ hadoop fs -rm -f hdfs://192.168.1.112:8020/*
2021-04-30 23:05:58,031 INFO Configuration.deprecation: No unit for dfs.client.datanode-restart.timeout(30) assuming SECONDS
rm: `hdfs://192.168.1.112:8020/tez': Is a directory

æ‰§è¡Œä»¥ä¸‹è¯­å¥
[chenbk@hadoop102 module]$ hadoop fs -rm -r hdfs://192.168.1.112:8020/*
2021-04-30 23:09:17,692 INFO Configuration.deprecation: No unit for dfs.client.datanode-restart.timeout(30) assuming SECONDS
Deleted hdfs://192.168.1.112:8020/tez
[chenbk@hadoop102 module]$

åœ¨å®¢æˆ·ç«¯æŸ¥çœ‹ï¼Œå·²ç»æ²¡æœ‰äº†è¿™ä¸ªæ–‡ä»¶ï¼ˆhttp://192.168.1.112:9870/explorer.html#/tezï¼‰

å†æ‰§è¡Œæ–°å»ºç›®å½•å‘½ä»¤
[chenbk@hadoop102 module]$ hadoop fs -mkdir /tez
2021-04-30 23:12:59,820 INFO Configuration.deprecation: No unit for dfs.client.datanode-restart.timeout(30) assuming SECONDS
[chenbk@hadoop102 module]$

æŠŠtez.tar.gzä¸Šä¼ åˆ°é›†ç¾¤ä¸­
è·¯å¾„ï¼š[chenbk@hadoop102 share]$ pwd
/opt/module/tez/share

æ‰§è¡Œä¸Šä¼ å‘½ä»¤
[chenbk@hadoop102 share]$ hadoop fs -put /opt/module/tez/share/tez.tar.gz /tez
2021-04-30 23:15:50,190 INFO Configuration.deprecation: No unit for dfs.client.datanode-restart.timeout(30) assuming SECONDS
2021-04-30 23:15:51,272 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
[chenbk@hadoop102 share]$

åœ¨åœ¨å®¢æˆ·ç«¯æŸ¥çœ‹ï¼Œæœ‰æ²¡æœ‰tez.tar.gz /tezè¿™ä¸ªæ–‡ä»¶

æ–°å»ºtez-site.xml
[chenbk@hadoop102 share]$ vim $HADOOP_HOME/etc/hadoop/tez-site.xml

æ·»åŠ ä»¥ä¸‹å†…å®¹
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
<property>
	<name>tez.lib.uris</name>
    <value>${fs.defaultFS}/tez/tez.tar.gz</value>
</property>
<property>
     <name>tez.use.cluster.hadoop-libs</name>
     <value>false</value>
</property>
<property>
     <name>tez.history.logging.service.class</name>
     <value>org.apache.tez.dag.history.logging.ats.ATSHistoryLoggingService</value>
</property></configuration>


ä¿®æ”¹Hadoopç¯å¢ƒå˜é‡
[chenbk@hadoop102 share]$ vim $HADOOP_HOME/etc/hadoop/hadoop-env.sh
æ·»åŠ ï¼šexport TEZ_CONF_DIR=$HADOOP_HOME/etc/hadoop
     export TEZ_JARS=/opt/module/tez
     export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:${TEZ_CONF_DIR}:${TEZ_JARS}/*:${TEZ_JARS}/lib/*

 å¦‚ä¸‹ï¼š

119 # An additional, custom CLASSPATH. Site-wide configs should be
120 # handled via the shellprofile functionality, utilizing the
121 # hadoop_add_classpath function for greater control and much
122 # harder for apps/end-users to accidentally override.
123 # Similarly, end users should utilize ${HOME}/.hadooprc .
124 # This variable should ideally only be used as a short-cut,
125 # interactive way for temporary additions on the command line.
126 # export HADOOP_CLASSPATH="/some/cool/path/on/your/machine"
127 export TEZ_CONF_DIR=$HADOOP_HOME/etc/hadoop
128 export TEZ_JARS=/opt/module/tez
129 export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:${TEZ_CONF_DIR}:${TEZ_JARS}/*:${TEZ_JARS}/lib/*
130

ä¿®æ”¹Hiveçš„è®¡ç®—å¼•æ“
vim $HIVE_HOME/conf/hive-site.xml
æ·»åŠ ï¼š
<property>
    <name>hive.execution.engine</name>
    <value>tez</value>
</property>

é˜²æ­¢æ—¥å¿—å†²çªï¼Œè§£å†³æ—¥å¿—JaråŒ…å†²çªï¼Œæ‰§è¡Œä»¥ä¸‹å‘½ä»¤
[chenbk@hadoop102 share]$ rm /opt/module/tez/lib/slf4j-log4j12-1.7.10.jar
[chenbk@hadoop102 share]$

é…ç½®å·²ç»å®Œæˆ

å¯åŠ¨Hive
å…ˆç™»é™†MySQL
mysql -uroot -p111111

[chenbk@hadoop102 share]$ mysql -uroot -p111111
mysql: [Warning] Using a password on the command line interface can be insecure.
Welcome to the MySQL monitor.  Commands end with ; or \g.
Your MySQL connection id is 4
Server version: 5.7.29 MySQL Community Server (GPL)

Copyright (c) 2000, 2020, Oracle and/or its affiliates. All rights reserved.

Oracle is a registered trademark of Oracle Corporation and/or its
affiliates. Other names may be trademarks of their respective
owners.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

mysql>

æ–°å»ºHiveå…ƒæ•°æ®åº“
mysql> create database metastore;
Query OK, 1 row affected (0.00 sec)

mysql>

é€€å‡ºquit;

åˆå§‹åŒ–Hiveå…ƒæ•°æ®åº“
æŸ¥çœ‹ä¸€ä¸‹ç›®å½•
[chenbk@hadoop102 share]$ cd /opt/module/hive/bin
[chenbk@hadoop102 bin]$ ll
æ€»ç”¨é‡ 44
-rwxr-xr-x. 1 chenbk chenbk   881 8æœˆ  22 2019 beeline
drwxrwxr-x. 3 chenbk chenbk  4096 4æœˆ  30 22:42 ext
-rwxr-xr-x. 1 chenbk chenbk 10158 8æœˆ  22 2019 hive
-rwxr-xr-x. 1 chenbk chenbk  1900 8æœˆ  22 2019 hive-config.sh
-rwxr-xr-x. 1 chenbk chenbk   885 8æœˆ  22 2019 hiveserver2
-rwxr-xr-x. 1 chenbk chenbk   880 8æœˆ  22 2019 hplsql
-rwxr-xr-x. 1 chenbk chenbk  3064 8æœˆ  22 2019 init-hive-dfs.sh
-rwxr-xr-x. 1 chenbk chenbk   832 8æœˆ  22 2019 metatool
-rwxr-xr-x. 1 chenbk chenbk   884 8æœˆ  22 2019 schematool

æ‰§è¡Œåˆå§‹åŒ–è¯­å¥
[chenbk@hadoop102 bin]$ schematool -initSchema -dbType mysql -verbose

0: jdbc:mysql://hadoop102:3306/metastore> /*!40101 SET CHARACTER_SET_CLIENT=@OLD_CHARACTER_SET_CLIENT */
No rows affected (0.001 seconds)
0: jdbc:mysql://hadoop102:3306/metastore> /*!40101 SET CHARACTER_SET_RESULTS=@OLD_CHARACTER_SET_RESULTS */
No rows affected (0.001 seconds)
0: jdbc:mysql://hadoop102:3306/metastore> /*!40101 SET COLLATION_CONNECTION=@OLD_COLLATION_CONNECTION */
No rows affected (0.001 seconds)
0: jdbc:mysql://hadoop102:3306/metastore> /*!40111 SET SQL_NOTES=@OLD_SQL_NOTES */
No rows affected (0.001 seconds)
0: jdbc:mysql://hadoop102:3306/metastore> !closeall
Closing: 0: jdbc:mysql://hadoop102:3306/metastore?useSSL=false
beeline>
beeline> Initialization script completed
schemaTool completed

åˆå§‹åŒ–å®Œæˆ


å¯åŠ¨metastoreå’Œhiveserver2

å…ˆç¼–è¾‘å¯åŠ¨è„šæœ¬
[chenbk@hadoop102 bin]$ vim $HIVE_HOME/bin/hiveservices.sh

æ·»åŠ ä»¥ä¸‹å†…å®¹
#!/bin/bash
HIVE_LOG_DIR=$HIVE_HOME/logs
META_PID=/tmp/meta.pid
SERVER_PID=/tmp/server.pid
mkdir -p $HIVE_LOG_DIR

function hive_start()
{
    nohup hive --service metastore >$HIVE_LOG_DIR/metastore.log 2>&1 &
    echo $! > $META_PID
    sleep 8
    nohup hive --service hiveserver2 >$HIVE_LOG_DIR/hiveserver2.log 2>&1 &
    echo $! > $SERVER_PID
}

function hive_stop()
{
    if [ -f $META_PID ]
    then
        cat $META_PID | xargs kill -9
        rm $META_PID
    else
        echo "Meta PIDæ–‡ä»¶ä¸¢å¤±ï¼Œè¯·æ‰‹åŠ¨å…³é—­æœåŠ¡"
    fi
    if [ -f $SERVER_PID ]
    then
        cat $SERVER_PID | xargs kill -9
        rm $SERVER_PID
    else
        echo "Server2 PIDæ–‡ä»¶ä¸¢å¤±ï¼Œè¯·æ‰‹åŠ¨å…³é—­æœåŠ¡"
    fi

}

case $1 in
"start")
    hive_start
    ;;
"stop")
    hive_stop
    ;;
"restart")
    hive_stop
    sleep 2
    hive_start
    ;;
*)
    echo Invalid Args!
    echo 'Usage: '$(basename $0)' start|stop|restart'
    ;;
esac


é‡åå­—
hive-exec-log4j2.properties.template 
è·¯å¾„ï¼š[chenbk@hadoop102 conf]$ pwd
/opt/module/hive/conf

æ‰§è¡Œå‘½ä»¤
[chenbk@hadoop102 conf]$ mv hive-log4j2.properties.template hive-log4j2.properties

æŸ¥çœ‹
[chenbk@hadoop102 conf]$ ll
æ€»ç”¨é‡ 336
-rw-r--r--. 1 chenbk chenbk   1596 8æœˆ  22 2019 beeline-log4j2.properties.template
-rw-r--r--. 1 chenbk chenbk 300482 8æœˆ  22 2019 hive-default.xml.template
-rw-r--r--. 1 chenbk chenbk   2365 8æœˆ  22 2019 hive-env.sh.template
-rw-r--r--. 1 chenbk chenbk   2274 8æœˆ  22 2019 hive-exec-log4j2.properties
-rw-r--r--. 1 chenbk chenbk   3086 8æœˆ  22 2019 hive-log4j2.properties
-rw-rw-r--. 1 chenbk chenbk   1552 4æœˆ  30 22:53 hive-site.xml
-rw-r--r--. 1 chenbk chenbk   2060 8æœˆ  22 2019 ivysettings.xml
-rw-r--r--. 1 chenbk chenbk   3558 8æœˆ  22 2019 llap-cli-log4j2.properties.template
-rw-r--r--. 1 chenbk chenbk   7163 8æœˆ  22 2019 llap-daemon-log4j2.properties.template
-rw-r--r--. 1 chenbk chenbk   2662 8æœˆ  22 2019 parquet-logging.properties
[chenbk@hadoop102 conf]$

æ‰§è¡Œä¿®æ”¹å‘½ä»¤
[chenbk@hadoop102 conf]$ vim hive-log4j2.properties

ä¿®æ”¹property.hive.log.dir = ${sys:java.io.tmpdir}/${sys:user.name}

#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

status = INFO
name = HiveLog4j2
packages = org.apache.hadoop.hive.ql.log

# list of properties
property.hive.log.level = INFO
property.hive.root.logger = DRFA
property.hive.log.dir = ${sys:java.io.tmpdir}/${sys:user.name}
property.hive.log.file = hive.log
property.hive.perflogger.log.level = INFO


ä¿®æ”¹ä¸º:property.hive.log.dir = /opt/module/hive/logs
status = INFO
name = HiveLog4j2
packages = org.apache.hadoop.hive.ql.log

# list of properties
property.hive.log.level = INFO
property.hive.root.logger = DRFA
property.hive.log.dir = /opt/module/hive/logs
property.hive.log.file = hive.log
property.hive.perflogger.log.level = INFO

æ·»åŠ æ‰§è¡Œæƒé™

[chenbk@hadoop102 conf]$ chmod +x $HIVE_HOME/bin/hiveservices.sh
[chenbk@hadoop102 conf]$

å¯åŠ¨Hiveåå°æœåŠ¡
[chenbk@hadoop102 bin]$ hiveservices.sh start
[chenbk@hadoop102 bin]$
è¿™æ˜¯æˆåŠŸçš„çŠ¶æ€

å¯åŠ¨å¤±è´¥çš„çŠ¶æ€å¦‚ä¸‹ï¼š
[chenbk@hadoop102 bin]$ hiveservices.sh start
mkdir: ç¼ºå°‘æ“ä½œæ•°
Try 'mkdir --help' for more information.
/opt/module/hive/bin/hiveservices.sh:è¡Œ7: /metastore.log: æƒé™ä¸å¤Ÿ
/opt/module/hive/bin/hiveservices.sh:è¡Œ10: /hiveserver2.log: æƒé™ä¸å¤Ÿ

è¿™æ˜¯hiveservices.sh æ²¡æœ‰é…ç½®å¥½ï¼Œéœ€è¦é‡æ–°é…ç½®ï¼Œå¯èƒ½æ˜¯è¯è¯­å°‘ä¸ªå­—æ¯

[chenbk@hadoop102 bin]$ hiveservices.sh start
/opt/module/hive/bin/hiveservices.sh:è¡Œ8: /opt/module/hive/logs/metastore.log: æ²¡æœ‰é‚£ä¸ªæ–‡ä»¶æˆ–ç›®å½•
/opt/module/hive/bin/hiveservices.sh:è¡Œ11: /opt/module/hive/logs/hiveserver2.log: æ²¡æœ‰é‚£ä¸ªæ–‡ä»¶æˆ–ç›®å½•

è¿™æ˜¯éœ€è¦åœ¨é…ç½®hiveservices.shï¼Œä¸­æ·»åŠ ï¼šmkdir -p $HIVE_LOG_DIR
æ­£ç¡®é…ç½®ï¼š
#!/bin/bash
HIVE_LOG_DIR=$HIVE_HOME/logs
META_PID=/tmp/meta.pid
SERVER_PID=/tmp/server.pid
mkdir -p $HIVE_LOG_DIR
function hive_start()
{
    nohup hive --service metastore >$HIVE_LOG_DIR/metastore.log 2>&1 &
echo $! > $META_PID
sleep 8
    nohup hive --service hiveserver2 >$HIVE_LOG_DIR/hiveserver2.log 2>&1 &
    echo $! > $SERVER_PID
}

function hive_stop()
{
    if [ -f $META_PID ]
    then
        cat $META_PID | xargs kill -9
        rm $META_PID
    else
        echo "Meta PIDæ–‡ä»¶ä¸¢å¤±ï¼Œè¯·æ‰‹åŠ¨å…³é—­æœåŠ¡"
    fi
    if [ -f $SERVER_PID ]
    then
        cat $SERVER_PID | xargs kill -9
        rm $SERVER_PID
    else
        echo "Server2 PIDæ–‡ä»¶ä¸¢å¤±ï¼Œè¯·æ‰‹åŠ¨å…³é—­æœåŠ¡"
    fi

}

case $1 in
"start")
    hive_start
    ;;
"stop")
    hive_stop
    ;;
"restart")
hive_stop
sleep 2
    hive_start
    ;;
*)
    echo Invalid Args!
    echo 'Usage: '$(basename $0)' start|stop|restart'
    ;;
esac

å¯åŠ¨å®Œæˆåï¼ŒæŸ¥çœ‹å¯åŠ¨çŠ¶æ€
[chenbk@hadoop102 ~]$ sh jpsall
=====  hadoop102   =====
3988 RunJar
2453 NameNode
2582 DataNode
4137 RunJar
2925 NodeManager
=====  hadoop103   =====
1620 NodeManager
1335 DataNode
1503 ResourceManager
=====  hadoop104   =====
1778 NodeManager
1573 DataNode
1642 SecondaryNameNode
[chenbk@hadoop102 ~]$

å¦‚æœæ‰¾åˆ°ä¸¤ä¸ªRunJarï¼ŒåŸºæœ¬å¯åŠ¨æˆåŠŸ
æŸ¥çœ‹ç«¯å£çŠ¶æ€
[chenbk@hadoop102 ~]$ netstat -nltp | grep -E 3988\|4137
(Not all processes could be identified, non-owned process info
 will not be shown, you would have to be root to see it all.)
tcp6       0      0 :::10000                :::*                    LISTEN      4137/java
tcp6       0      0 :::10002                :::*                    LISTEN      4137/java
tcp6       0      0 :::9083                 :::*                    LISTEN      3988/java

è¯æ˜å¯åŠ¨æˆåŠŸ

æŸ¥çœ‹ç«¯å£å‘½ä»¤ï¼š
[chenbk@hadoop102 ~]$ netstat -nltp

(Not all processes could be identified, non-owned process info
 will not be shown, you would have to be root to see it all.)
Active Internet connections (only servers)
Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name
tcp        0      0 127.0.0.1:42439         0.0.0.0:*               LISTEN      2582/java
tcp        0      0 0.0.0.0:8040            0.0.0.0:*               LISTEN      2925/java
tcp        0      0 0.0.0.0:9864            0.0.0.0:*               LISTEN      2582/java
tcp        0      0 0.0.0.0:8042            0.0.0.0:*               LISTEN      2925/java
tcp        0      0 0.0.0.0:9866            0.0.0.0:*               LISTEN      2582/java
tcp        0      0 0.0.0.0:9867            0.0.0.0:*               LISTEN      2582/java
tcp        0      0 0.0.0.0:9870            0.0.0.0:*               LISTEN      2453/java
tcp        0      0 192.168.1.112:8020      0.0.0.0:*               LISTEN      2453/java
tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN      -
tcp        0      0 127.0.0.1:25            0.0.0.0:*               LISTEN      -
tcp        0      0 0.0.0.0:13562           0.0.0.0:*               LISTEN      2925/java
tcp        0      0 0.0.0.0:35423           0.0.0.0:*               LISTEN      2925/java
tcp6       0      0 :::3306                 :::*                    LISTEN      -
tcp6       0      0 :::10000                :::*                    LISTEN      4137/java
tcp6       0      0 :::10002                :::*                    LISTEN      4137/java
tcp6       0      0 :::22                   :::*                    LISTEN      -
tcp6       0      0 ::1:25                  :::*                    LISTEN      -
tcp6       0      0 :::9083                 :::*                    LISTEN      3988/java
[chenbk@hadoop102 ~]$

å¯åŠ¨beelineå®¢æˆ·ç«¯

beeline -u jdbc:hive2://hadoop102:10000 -n chenbk

[chenbk@hadoop102 ~]$ beeline -u jdbc:hive2://hadoop102:10000 -n chenbk
Connecting to jdbc:hive2://hadoop102:10000
Connected to: Apache Hive (version 3.1.2)
Driver: Hive JDBC (version 3.1.2)
Transaction isolation: TRANSACTION_REPEATABLE_READ
Beeline version 3.1.2 by Apache Hive
0: jdbc:hive2://hadoop102:10000>

å¯åŠ¨æˆåŠŸ

æŸ¥çœ‹è¡¨ï¼š
0: jdbc:hive2://hadoop102:10000> show table;

Error: Error while compiling statement: FAILED: ParseException line 1:10 mismatched input '<EOF>' expecting EXTENDED near 'table' in show statement (state=42000,code=40000)
0: jdbc:hive2://hadoop102:10000> show tables;
INFO  : Compiling command(queryId=chenbk_20210501002654_08dc33cb-784c-4711-a348-6c8556e79b8d): show tables
INFO  : Concurrency mode is disabled, not creating a lock manager
INFO  : Semantic Analysis Completed (retrial = false)
INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:tab_name, type:string, comment:from deserializer)], properties:null)
INFO  : Completed compiling command(queryId=chenbk_20210501002654_08dc33cb-784c-4711-a348-6c8556e79b8d); Time taken: 0.268 seconds
INFO  : Concurrency mode is disabled, not creating a lock manager
INFO  : Executing command(queryId=chenbk_20210501002654_08dc33cb-784c-4711-a348-6c8556e79b8d): show tables
INFO  : Starting task [Stage-0:DDL] in serial mode
INFO  : Completed executing command(queryId=chenbk_20210501002654_08dc33cb-784c-4711-a348-6c8556e79b8d); Time taken: 0.055 seconds
INFO  : OK
INFO  : Concurrency mode is disabled, not creating a lock manager
+-----------+
| tab_name  |
+-----------+
+-----------+
No rows selected (0.621 seconds)
0: jdbc:hive2://hadoop102:10000>

åˆ›å»ºè¡¨
0: jdbc:hive2://hadoop102:10000> create table chenbk(id int,value string);
INFO  : Compiling command(queryId=chenbk_20210501002918_edcf8a42-9fc4-43c9-84ec-7deba5413bcd): create table chenbk(id int,value string)
INFO  : Concurrency mode is disabled, not creating a lock manager
INFO  : Semantic Analysis Completed (retrial = false)
INFO  : Returning Hive schema: Schema(fieldSchemas:null, properties:null)
INFO  : Completed compiling command(queryId=chenbk_20210501002918_edcf8a42-9fc4-43c9-84ec-7deba5413bcd); Time taken: 0.125 seconds
INFO  : Concurrency mode is disabled, not creating a lock manager
INFO  : Executing command(queryId=chenbk_20210501002918_edcf8a42-9fc4-43c9-84ec-7deba5413bcd): create table chenbk(id int,value string)
INFO  : Starting task [Stage-0:DDL] in serial mode
INFO  : Completed executing command(queryId=chenbk_20210501002918_edcf8a42-9fc4-43c9-84ec-7deba5413bcd); Time taken: 1.267 seconds
INFO  : OK
INFO  : Concurrency mode is disabled, not creating a lock manager
No rows affected (1.68 seconds)

æ’å…¥æ•°æ® 
0: jdbc:hive2://hadoop102:10000> insert into table chenbk values(1001,"kang");
INFO  : Compiling command(queryId=chenbk_20210501003049_faacc8be-e937-4d07-ae2d-c7e4740b1413): insert into table chenbk values(1001,"kang")
INFO  : Concurrency mode is disabled, not creating a lock manager
INFO  : Semantic Analysis Completed (retrial = false)
INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:col1, type:int, comment:null), FieldSchema(name:col2, type:string, comment:null)], properties:null)
INFO  : Completed compiling command(queryId=chenbk_20210501003049_faacc8be-e937-4d07-ae2d-c7e4740b1413); Time taken: 3.244 seconds
INFO  : Concurrency mode is disabled, not creating a lock manager
INFO  : Executing command(queryId=chenbk_20210501003049_faacc8be-e937-4d07-ae2d-c7e4740b1413): insert into table chenbk values(1001,"kang")
INFO  : Query ID = chenbk_20210501003049_faacc8be-e937-4d07-ae2d-c7e4740b1413
INFO  : Total jobs = 1
INFO  : Launching Job 1 out of 1
INFO  : Starting task [Stage-1:MAPRED] in serial mode
INFO  : Subscribed to counters: [] for queryId: chenbk_20210501003049_faacc8be-e937-4d07-ae2d-c7e4740b1413
INFO  : Tez session hasn't been created yet. Opening session
INFO  : Dag name: insert into table chenbk values(1001,"kang") (Stage-1)
INFO  : Status: Running (Executing on YARN cluster with App id application_1619838125334_0002)

----------------------------------------------------------------------------------------------
        VERTICES      MODE        STATUS  TOTAL  COMPLETED  RUNNING  PENDING  FAILED  KILLED
----------------------------------------------------------------------------------------------
Map 1 .......... container     SUCCEEDED      1          1        0        0       0       0
Reducer 2 ...... container     SUCCEEDED      1          1        0        0       0       0
----------------------------------------------------------------------------------------------
VERTICES: 02/02  [==========================>>] 100%  ELAPSED TIME: 17.06 s
----------------------------------------------------------------------------------------------
INFO  : Starting task [Stage-2:DEPENDENCY_COLLECTION] in serial mode
INFO  : Starting task [Stage-0:MOVE] in serial mode
INFO  : Loading data to table default.chenbk from hdfs://hadoop102:8020/user/hive/warehouse/chenbk/.hive-staging_hive_2021-05-01_00-30-49_082_7775360843546126635-1/-ext-10000
INFO  : Starting task [Stage-3:STATS] in serial mode
INFO  : Completed executing command(queryId=chenbk_20210501003049_faacc8be-e937-4d07-ae2d-c7e4740b1413); Time taken: 28.01 seconds
INFO  : OK
INFO  : Concurrency mode is disabled, not creating a lock manager
No rows affected (31.283 seconds)
0: jdbc:hive2://hadoop102:10000>

é€€å‡ºç™»å½•
ctrl+c


zookeeper å®‰è£…
å…ˆä¸Šä¼ æ–‡ä»¶åˆ°é›†ç¾¤
è·¯å¾„æ˜¯ï¼š[chenbk@hadoop102 software]$ pwd
/opt/software

è§£å‹æ–‡ä»¶åˆ°module/
[chenbk@hadoop102 software]$ tar -zvxf apache-zookeeper-3.5.7-bin.tar.gz -C /opt/module/


é‡æ–°å‘½å
[chenbk@hadoop102 module]$ mv apache-zookeeper-3.5.7-bin/ zookeeper

é…ç½®ç¯å¢ƒå˜é‡
[chenbk@hadoop102 module]$ vim /etc/profile.d/my_env.sh

æ·»åŠ ä»¥ä¸‹å†…å®¹
#ZOOKEEPER_HOME
export ZOOKEEPER_HOME=/opt/module/zookeeper
export PATH=$PATH:$ZOOKEEPER_HOME/bin

å…±äº¨åˆ°å…¶å®ƒé›†ç¾¤
sh xsync /etc/profile.d

éªŒè¯ z + 	tabé”®
[chenbk@hadoop102 ~]$ z
zcat             zdump            zforce           zless            znew
zcmp             zegrep           zgrep            zlib_decompress  zramctl
zdiff            zfgrep           zic              zmore            zsoelim
[chenbk@hadoop102 ~]$


é…ç½®æ–‡ä»¶
zoo.cfg(é‡å‘½åå)

è·¯å¾„ï¼š[chenbk@hadoop102 conf]$ pwd
/opt/module/zookeeper/conf

[chenbk@hadoop102 conf]$ vim zoo.cfg


æŠŠdataDir=ï¼Œä¿®æ”¹ä¸ºï¼šdataDir=/opt/module/zookeeper/zkData
# the directory where the snapshot is stored.
# do not use /tmp for storage, /tmp here is just
# example sakes.
dataDir=/opt/module/zookeeper/zkData

åœ¨æœ€åæ·»åŠ 
# Purge task interval in hours
# Set to "0" to disable auto purge feature
#autopurge.purgeInterval=1
server.2=hadoop102:2888:3888
server.3=hadoop103:2888:3888
server.4=hadoop104:2888:3888


å¯åŠ¨å•æœºæ¨¡å¼
[chenbk@hadoop102 zookeeper]$ zkServer.sh start
-bash: /opt/module/zookeeper/bin/zkServer.sh: æƒé™ä¸å¤Ÿ
å½“æƒé™ä¸å¤Ÿæ—¶
æ‰§è¡Œä»¥ä¸‹è¯­å¥[chenbk@hadoop102 bin]$ chmod +x zkServer.sh
è·¯å¾„ä¸ºï¼š[chenbk@hadoop102 bin]$ pwd
/opt/module/zookeeper/bin
[chenbk@hadoop102 bin]$

å†æ‰§è¡Œ
[chenbk@hadoop102 bin]$ zkServer.sh start
ZooKeeper JMX enabled by default
Using config: /opt/module/zookeeper/bin/../conf/zoo.cfg
Starting zookeeper ... STARTED

å†æ‰§è¡Œ
[chenbk@hadoop102 bin]$ zkServer.sh status

[chenbk@hadoop102 ~]$ sh jpsall
=====  hadoop102   =====
3988 RunJar
2453 NameNode
2582 DataNode
4137 RunJar
2925 NodeManager
5039 QuorumPeerMain
=====  hadoop103   =====
1620 NodeManager
1335 DataNode
1503 ResourceManager
=====  hadoop104   =====
1778 NodeManager
1573 DataNode
1642 SecondaryNameNode

å†æ‰§è¡Œ
[chenbk@hadoop102 ~]$ jps -l
5168 sun.tools.jps.Jps
3988 org.apache.hadoop.util.RunJar
2453 org.apache.hadoop.hdfs.server.namenode.NameNode
2582 org.apache.hadoop.hdfs.server.datanode.DataNode
4137 org.apache.hadoop.util.RunJar
2925 org.apache.hadoop.yarn.server.nodemanager.NodeManager
5039 org.apache.zookeeper.server.quorum.QuorumPeerMain
[chenbk@hadoop102 ~]$

å…¶ä¸­ï¼š5039 org.apache.zookeeper.server.quorum.QuorumPeerMain  
å®‰è£…æˆåŠŸ

å•æœºæ¨¡å¼å°‘ç”¨ï¼Œéƒ½ä¸ºé›†ç¾¤æ¨¡å¼
æ‰€ä»¥ï¼šå…ˆå…³é—­
[chenbk@hadoop102 zookeeper]$ zkServer.sh stop

[chenbk@hadoop102 zookeeper]$ zkServer.sh stop
ZooKeeper JMX enabled by default
Using config: /opt/module/zookeeper/bin/../conf/zoo.cfg
Stopping zookeeper ... STOPPED


æŸ¥çœ‹æ˜¯å¦å·²å…³é—­
[chenbk@hadoop102 zookeeper]$ jps
5233 Jps
3988 RunJar
2453 NameNode
2582 DataNode
4137 RunJar
2925 NodeManag


åˆ é™¤zkDataæ–‡ä»¶çš„å†…å®¹
[chenbk@hadoop102 zookeeper]$ rm -rf zkData/*

åœ¨zkData æ–‡ä»¶ä¸­æ–°å»ºä¸€ä¸ªæ–‡ä»¶
[chenbk@hadoop102 zkData]$ vim myid
æ·»åŠ çš„å†…å®¹ï¼š2


åŒæ­¥åˆ°å…¶å®ƒé›†ç¾¤
æ‰§è¡Œå‘½ä»¤
[chenbk@hadoop102 ~]$ sh xsync /opt/module/zookeeper/

åœ¨hadoop103/hadoop104ä¸­ï¼Œä¿®æ”¹myid

[chenbk@hadoop103 zkData]$ vim myid
[chenbk@hadoop103 zkData]$

[chenbk@hadoop104 zkData]$ vim myid
[chenbk@hadoop104 zkData]$


åˆ†åˆ«æ”¹ä¸ºï¼š3 4

å¯ä»¥è¿è¡Œä»¥ä¸‹å‘½ä»¤ä¿®æ”¹
[chenbk@hadoop102 zkData]$ echo 4 > $ZOOKEEPER_HOME/zkData/myid
[chenbk@hadoop102 zkData]$ cat myid
4
[chenbk@hadoop102 zkData]$ echo 2 > $ZOOKEEPER_HOME/zkData/myid
[chenbk@hadoop102 zkData]$ cat myid
2
[chenbk@hadoop102 zkData]$


å¯åŠ¨
[chenbk@hadoop102 zookeeper]$ zkServer.sh start
ZooKeeper JMX enabled by default
Using config: /opt/module/zookeeper/bin/../conf/zoo.cfg
Starting zookeeper ... STARTED

å†æ‰§è¡Œå‘½ä»¤

[chenbk@hadoop102 zookeeper]$ zkServer.sh status
ZooKeeper JMX enabled by default
Using config: /opt/module/zookeeper/bin/../conf/zoo.cfg
Client port found: 2181. Client address: localhost.
Error contacting service. It is probably not running.
[chenbk@hadoop102 zookeeper]$

çŠ¶æ€ä¸å¯¹ï¼Œæ˜¾ç¤ºå‡ºé”™
Client port found: 2181. Client address: localhost.
Error contacting service. It is probably not running.


åŸå› ï¼šæœ€å°‘éœ€è¦ä¸¤é›†ç¾¤å¯åŠ¨
æ‰€ä»¥å†å¯åŠ¨hadoop103

[chenbk@hadoop103 zkData]$ zkServer.sh start
ZooKeeper JMX enabled by default
Using config: /opt/module/zookeeper/bin/../conf/zoo.cfg
Starting zookeeper ... STARTED
[chenbk@hadoop103 zkData]$

å†æ‰§è¡Œå‘½ä»¤

[chenbk@hadoop102 zookeeper]$ zkServer.sh status
ZooKeeper JMX enabled by default
Using config: /opt/module/zookeeper/bin/../conf/zoo.cfg
Client port found: 2181. Client address: localhost.
Mode: follower
[chenbk@hadoop102 zookeeper]$

æ˜¾ç¤ºæˆåŠŸ

åˆ é™¤è¿›ç¨‹ kill -9 5351
[chenbk@hadoop102 zookeeper]$ jps
5489 Jps
3988 RunJar
2453 NameNode
2582 DataNode
5351 QuorumPeerMain
4137 RunJar
2925 NodeManager
[chenbk@hadoop102 zookeeper]$ kill -9 5351
[chenbk@hadoop102 zookeeper]$


æŸ¥çœ‹
[chenbk@hadoop102 zookeeper]$ jps
3988 RunJar
2453 NameNode
2582 DataNode
5511 Jps
4137 RunJar
2925 NodeManager
[chenbk@hadoop102 zookeeper]$




å®‰è£…Flume

ä¸Šä¼ æ–‡ä»¶åˆ°é›†ç¾¤

è§£å‹æ–‡ä»¶åˆ°/opt/module/
[chenbk@hadoop102 software]$ tar -zvxf apache-flume-1.9.0-bin.tar -C /opt/module/

gzip: stdin: not in gzip format
tar: Child returned status 1
tar: Error is not recoverable: exiting now

å‘ç°è§£å‹ä¸æˆåŠŸ
tarç”¨tar -xfæ¥è§£å‹
[chenbk@hadoop102 software]$ tar -xf apache-flume-1.9.0-bin.tar -C /opt/module/

æŸ¥çœ‹
[chenbk@hadoop102 software]$ tar -xf apache-flume-1.9.0-bin.tar -C /opt/module/
[chenbk@hadoop102 software]$ cd /opt/module/
[chenbk@hadoop102 module]$ ll
æ€»ç”¨é‡ 4
drwxrwxr-x.  7 chenbk chenbk  187 5æœˆ   1 02:41 apache-flume-1.9.0-bin
drwxr-xr-x. 11 chenbk chenbk  173 4æœˆ  30 12:05 hadoop-3.1.3
drwxrwxr-x. 11 chenbk chenbk  196 5æœˆ   1 00:12 hive
drwxr-xr-x.  7 chenbk chenbk  245 4æœˆ   2 2019 jdk1.8.0_212
drwxrwxr-x.  7 chenbk chenbk  101 4æœˆ  30 12:37 kafka
drwxr-xr-x.  5 chenbk chenbk 4096 3æœˆ  19 2019 tez
drwxrwxr-x.  8 chenbk chenbk  160 5æœˆ   1 02:03 zookeeper
[chenbk@hadoop102 module]$

ä¿®æ”¹åç§°
[chenbk@hadoop102 module]$ mv apache-flume-1.9.0-bin flume

å› ç‰ˆæœ¬ä¸º1.9.0
æ‰€ä»¥éœ€è¦å°†libæ–‡ä»¶ä¸‹guava-11.0.2.jaråˆ é™¤ï¼Œä¸ºäº†å…¼å®¹Hadoop3.1.3

æ‰§è¡Œä»¥ä¸‹å‘½ä»¤
[chenbk@hadoop102 module]$ rm /opt/module/flume/lib/guava-11.0.2.jar
[chenbk@hadoop102 module]$

é…ç½®ç¯å¢ƒå˜é‡
[chenbk@hadoop102 conf]$ vim /etc/profile.d/my_env.sh

æ·»åŠ ä»¥ä¸‹å†…å®¹

#FLUME_HOME
export FLUME_HOME=/opt/module/flume
export PATH=$PATH:$FLUME_HOME/bin


éªŒè¯
flume + tab
[chenbk@hadoop102 module]$ flume
flume         flume-ng      flume-ng.ps1
[chenbk@hadoop102 module]$


ç¾¤å‘
[chenbk@hadoop102 ~]$ sh xsync /opt/module/flume/

æ‰§è¡Œä¸€ä¸‹
[chenbk@hadoop102 ~]$ flume-ng

Usage: /opt/module/flume/bin/flume-ng <command> [options]...

commands:
  help                      display this help text
  agent                     run a Flume agent
  avro-client               run an avro Flume client
  version                   show Flume version info

global options:
  --conf,-c <conf>          use configs in <conf> directory
  --classpath,-C <cp>       append to the classpath
  --dryrun,-d               do not actually start Flume, just print the command
  --plugins-path <dirs>     colon-separated list of plugins.d directories. See the
                            plugins.d section in the user guide for more details.
                            Default: $FLUME_HOME/plugins.d
  -Dproperty=value          sets a Java system property value
  -Xproperty=value          sets a Java -X option

agent options:
  --name,-n <name>          the name of this agent (required)
  --conf-file,-f <file>     specify a config file (required if -z missing)
  --zkConnString,-z <str>   specify the ZooKeeper connection to use (required if -f missing)
  --zkBasePath,-p <path>    specify the base path in ZooKeeper for agent configs
  --no-reload-conf          do not reload config file if changed
  --help,-h                 display help text

avro-client options:
  --rpcProps,-P <file>   RPC client properties file with server connection params
  --host,-H <host>       hostname to which events will be sent
  --port,-p <port>       port of the avro source
  --dirname <dir>        directory to stream to avro source
  --filename,-F <file>   text file to stream to avro source (default: std input)
  --headerFile,-R <file> File containing event headers as key/value pairs on each new line
  --help,-h              display help text

  Either --rpcProps or both --host and --port must be specified.

Note that if <conf> directory is specified, then it is always included first
in the classpath.


å®‰è£…å®Œæˆ


å®‰è£…kafka
ä¸Šä¼ æ–‡ä»¶åˆ°é›†ç¾¤

è§£å‹
[chenbk@hadoop102 software]$ tar -zxvf kafka_2.11-2.4.1.tgz -C /opt/module/

é‡å‘½å
[chenbk@hadoop102 module]$ mv kafka_2.11-2.4.1 kafka

é…ç½®ç¯å¢ƒå˜é‡

[chenbk@hadoop102 module]$ vim /etc/profile.d/my_env.sh

æ·»åŠ ä»¥ä¸‹å†…å®¹

#KAFKA_HOME
export KAFKA_HOME=/opt/module/kafka
export PATH=$PATH:$KAFKA_HOME/bin


éªŒè¯ k+tab

[chenbk@hadoop102 module]$ k
kafka-acls.sh                        kafka-preferred-replica-election.sh  kbdrate
kafka-broker-api-versions.sh         kafka-producer-perf-test.sh          kdumpctl
kafka-configs.sh                     kafka-reassign-partitions.sh         kernel-install
kafka-console-consumer.sh            kafka-replica-verification.sh        kexec
kafka-console-producer.sh            kafka-run-class.sh                   keytool
kafka-consumer-groups.sh             kafka-server-start.sh                kill
kafka-consumer-perf-test.sh          kafka-server-stop.sh                 killall
kafka-delegation-tokens.sh           kafka-streams-application-reset.sh   killall5
kafka-delete-records.sh              kafka-topics.sh                      kmod
kafka-dump-log.sh                    kafka-verifiable-consumer.sh         kms.sh
kafka-leader-election.sh             kafka-verifiable-producer.sh         kpartx
kafka-log-dirs.sh                    kbdinfo                              krb5-config
kafka-mirror-maker.sh                kbd_mode
[chenbk@hadoop102 module]$ k


é…ç½®kafka

[chenbk@hadoop102 config]$ vim server.properties

æŸ¥æ‰¾å†…å®¹ï¼šcommand + f
ä¿®æ”¹çš„å†…å®¹
1ã€
 20 # The id of the broker. This must be set to a unique integer for each broker.
 21 broker.id=2
 22

2ã€ 
 58 # A comma separated list of directories under which to store log files
 59 log.dirs=/tmp/kafka-logs 
ä¿®æ”¹ä¸ºï¼š
 58 # A comma separated list of directories under which to store log files
 59 log.dirs=/opt/module/kafka/logs

3ã€
117 # Zookeeper connection string (see zookeeper docs for details).
118 # This is a comma separated host:port pairs, each corresponding to a zk
119 # server. e.g. "127.0.0.1:3000,127.0.0.1:3001,127.0.0.1:3002".
120 # You can also append an optional chroot string to the urls to specify the
121 # root directory for all kafka znodes.
122 zookeeper.connect=localhost:2181
123

ä¿®æ”¹ä¸ºï¼š
120 # You can also append an optional chroot string to the urls to specify the
121 # root directory for all kafka znodes.
122 zookeeper.connect=192.168.1.112:2181,192.168.1.113:2181,192.168.1.114:2181
123

åˆ†å‘é›†ç¾¤
[chenbk@hadoop102 ~]$ sh xsync /opt/module/kafka/

åˆ†å‘ç¯å¢ƒå˜é‡
[chenbk@hadoop102 ~]$ sh xsync /etc/profile.d/


å…ˆå¯åŠ¨zookeeper
[chenbk@hadoop102 ~]$ zkServer.sh start
ZooKeeper JMX enabled by default
Using config: /opt/module/zookeeper/bin/../conf/zoo.cfg
Starting zookeeper ... STARTED

[chenbk@hadoop102 ~]$ zkServer.sh status
ZooKeeper JMX enabled by default
Using config: /opt/module/zookeeper/bin/../conf/zoo.cfg
Client port found: 2181. Client address: localhost.
Mode: follower
[chenbk@hadoop102 ~]$

å¯åŠ¨é›†ç¾¤
[chenbk@hadoop102 kafka]$ kafka-server-start.sh -daemon $KAFKA_HOME/config/server.properties
[chenbk@hadoop102 kafka]$



é›†ç¾¤é‡å¯åï¼Œéœ€è¦æ‰§è¡Œçš„å‘½ä»¤
hadoopå‘½ä»¤
[chenbk@hadoop102 hadoop-3.1.3]$ sbin/start-dfs.sh
Starting namenodes on [hadoop102]
Starting datanodes
Starting secondary namenodes [hadoop104]
[chenbk@hadoop102 hadoop-3.1.3]$

[chenbk@hadoop103 hadoop-3.1.3]$ sbin/start-yarn.sh
Starting resourcemanager
Starting nodemanagers
[chenbk@hadoop103 hadoop-3.1.3]$


å¯åŠ¨zookeeper
[chenbk@hadoop102 hadoop-3.1.3]$ sbin/start-dfs.sh
Starting namenodes on [hadoop102]
Starting datanodes
Starting secondary namenodes [hadoop104]
[chenbk@hadoop102 hadoop-3.1.3]$ zkServer.sh start
ZooKeeper JMX enabled by default
Using config: /opt/module/zookeeper/bin/../conf/zoo.cfg
Starting zookeeper ... STARTED
[chenbk@hadoop102 hadoop-3.1.3]$ zkServer.sh status
ZooKeeper JMX enabled by default
Using config: /opt/module/zookeeper/bin/../conf/zoo.cfg
Client port found: 2181. Client address: localhost.
Mode: leader
[chenbk@hadoop102 hadoop-3.1.3]$

[chenbk@hadoop103 hadoop-3.1.3]$ sbin/start-yarn.sh
Starting resourcemanager
Starting nodemanagers
[chenbk@hadoop103 hadoop-3.1.3]$ zkServer.sh start
ZooKeeper JMX enabled by default
Using config: /opt/module/zookeeper/bin/../conf/zoo.cfg
Starting zookeeper ... STARTED
[chenbk@hadoop103 hadoop-3.1.3]$ zkServer.sh status
ZooKeeper JMX enabled by default
Using config: /opt/module/zookeeper/bin/../conf/zoo.cfg
Client port found: 2181. Client address: localhost.
Mode: follower
[chenbk@hadoop103 hadoop-3.1.3]$


[chenbk@hadoop104 ~]$ zkServer.sh start
ZooKeeper JMX enabled by default
Using config: /opt/module/zookeeper/bin/../conf/zoo.cfg
Starting zookeeper ... STARTED
[chenbk@hadoop104 ~]$ zkServer.sh status
ZooKeeper JMX enabled by default
Using config: /opt/module/zookeeper/bin/../conf/zoo.cfg
Client port found: 2181. Client address: localhost.
Mode: follower
[chenbk@hadoop104 ~]$


å†å¯åŠ¨kafa

[chenbk@hadoop103 hadoop-3.1.3]$ kafka-server-start.sh -daemon $KAFKA_HOME/config/server.properties
[chenbk@hadoop103 hadoop-3.1.3]$

[chenbk@hadoop102 hadoop-3.1.3]$ kafka-server-start.sh -daemon $KAFKA_HOME/config/server.properties
[chenbk@hadoop102 hadoop-3.1.3]$

[chenbk@hadoop104 ~]$ kafka-server-start.sh -daemon $KAFKA_HOME/config/server.properties
[chenbk@hadoop104 ~]$

æŸ¥çœ‹è¿›ç¨‹
[chenbk@hadoop102 ~]$ sh jpsall
=====  hadoop102   =====
2433 QuorumPeerMain
1922 DataNode
2279 NodeManager
2875 Kafka
1790 NameNode
=====  hadoop103   =====
1953 QuorumPeerMain
1462 ResourceManager
1580 NodeManager
1294 DataNode
=====  hadoop104   =====
2340 Kafka
1911 QuorumPeerMain
1768 NodeManager
1561 DataNode
1631 SecondaryNameNode
[chenbk@hadoop102 ~]$


æŸ¥çœ‹èŠ‚ç‚¹

/zookeeper/bin/../lib/commons-cli-1.2.jar:/opt/module/zookeeper/bin/../lib/audience-annotations-0.5.0.jar:/opt/module/zookeeper/bin/../zookeeper-*.jar:/opt/module/zookeeper/bin/../zookeeper-server/src/main/resources/lib/*.jar:/opt/module/zookeeper/bin/../conf:
2021-05-01 04:05:39,529 [myid:] - INFO  [main:Environment@109] - Client environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib
2021-05-01 04:05:39,529 [myid:] - INFO  [main:Environment@109] - Client environment:java.io.tmpdir=/tmp
2021-05-01 04:05:39,529 [myid:] - INFO  [main:Environment@109] - Client environment:java.compiler=<NA>
2021-05-01 04:05:39,530 [myid:] - INFO  [main:Environment@109] - Client environment:os.name=Linux
2021-05-01 04:05:39,530 [myid:] - INFO  [main:Environment@109] - Client environment:os.arch=amd64
2021-05-01 04:05:39,530 [myid:] - INFO  [main:Environment@109] - Client environment:os.version=3.10.0-1160.el7.x86_64
2021-05-01 04:05:39,530 [myid:] - INFO  [main:Environment@109] - Client environment:user.name=chenbk
2021-05-01 04:05:39,530 [myid:] - INFO  [main:Environment@109] - Client environment:user.home=/home/chenbk
2021-05-01 04:05:39,530 [myid:] - INFO  [main:Environment@109] - Client environment:user.dir=/opt/module/zookeeper/bin
2021-05-01 04:05:39,531 [myid:] - INFO  [main:Environment@109] - Client environment:os.memory.free=52MB
2021-05-01 04:05:39,533 [myid:] - INFO  [main:Environment@109] - Client environment:os.memory.max=228MB
2021-05-01 04:05:39,533 [myid:] - INFO  [main:Environment@109] - Client environment:os.memory.total=57MB
2021-05-01 04:05:39,538 [myid:] - INFO  [main:ZooKeeper@868] - Initiating client connection, connectString=localhost:2181 sessionTimeout=30000 watcher=org.apache.zookeeper.ZooKeeperMain$MyWatcher@5fcfe4b2
2021-05-01 04:05:39,546 [myid:] - INFO  [main:X509Util@79] - Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation
2021-05-01 04:05:39,556 [myid:] - INFO  [main:ClientCnxnSocket@237] - jute.maxbuffer value is 4194304 Bytes
2021-05-01 04:05:39,568 [myid:] - INFO  [main:ClientCnxn@1653] - zookeeper.request.timeout value is 0. feature enabled=
Welcome to ZooKeeper!
2021-05-01 04:05:39,594 [myid:localhost:2181] - INFO  [main-SendThread(localhost:2181):ClientCnxn$SendThread@1112] - Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
JLine support is enabled
2021-05-01 04:05:39,686 [myid:localhost:2181] - INFO  [main-SendThread(localhost:2181):ClientCnxn$SendThread@959] - Socket connection established, initiating session, client: /127.0.0.1:48118, server: localhost/127.0.0.1:2181
2021-05-01 04:05:39,756 [myid:localhost:2181] - INFO  [main-SendThread(localhost:2181):ClientCnxn$SendThread@1394] - Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x200000431790000, negotiated timeout = 30000

WATCHER::

WatchedEvent state:SyncConnected type:None path:null

æ‰§è¡Œè¯­å¥
[zk: localhost:2181(CONNECTED) 0] ls /

[zk: localhost:2181(CONNECTED) 0] ls /
[admin, brokers, cluster, config, consumers, controller, controller_epoch, isr_change_notification, latest_producer_id_block, log_dir_event_notification, zookeeper]

æŸ¥çœ‹è¯­å¥
[zk: localhost:2181(CONNECTED) 1] ls /brokers
[ids, seqid, topics]

[zk: localhost:2181(CONNECTED) 2] ls /brokers/ids
[2, 4]
[zk: localhost:2181(CONNECTED) 3]


æ³¨æ„ï¼š103æ— æ³•å¯åŠ¨ï¼Œæš‚æ—¶ä¸ç”¨ç®¡
å·²ç»å®‰è£…å®Œæˆ


å®‰è£…spark

è§£å‹åˆ°ç›¸åº”çš„æ–‡ä»¶ä¸­
[chenbk@hadoop102 software]$ tar -zvxf spark-2.1.1-bin-hadoop2.7.tgz -C /opt/module/

é‡å‘½å
[chenbk@hadoop102 bin]$ mv spark-2.1.1-bin-hadoop2.7 spark-local

è¿è¡Œæ¡ˆä¾‹ï¼Œæ±‚Ï€
æ‰§è¡Œå‘½ä»¤

[chenbk@hadoop102 spark-local]$ bin/spark-submit \
> --class org.apache.spark.examples.SparkPi \
> --master local[2] \
> ./examples/jars/spark-examples_2.11-2.1.1.jar \
> 10

ç»“æœ
Pi is roughly 3.1424711424711425
21/05/01 04:24:18 INFO SparkUI: Stopped Spark web UI at http://192.168.1.112:4040
21/05/01 04:24:18 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
21/05/01 04:24:19 INFO MemoryStore: MemoryStore cleared
21/05/01 04:24:19 INFO BlockManager: BlockManager stopped
21/05/01 04:24:19 INFO BlockManagerMaster: BlockManagerMaster stopped
21/05/01 04:24:19 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
21/05/01 04:24:19 INFO SparkContext: Successfully stopped SparkContext
21/05/01 04:24:19 INFO ShutdownHookManager: Shutdown hook called
21/05/01 04:24:19 INFO ShutdownHookManager: Deleting directory /tmp/spark-b16e979e-4ee0-415f-90d3-ae3d8df2e475
[chenbk@hadoop102 spark-local]$

å®˜æ–¹WordCountæ¡ˆä¾‹

å¯åŠ¨spark-shell

æ‰§è¡Œå¦‚ä¸‹å‘½ä»¤
[chenbk@hadoop102 spark-local]$ bin/spark-shell
å¾—åˆ°ç»“æœ

[chenbk@hadoop102 spark-local]$ bin/spark-shell
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
21/05/01 04:28:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
21/05/01 04:29:11 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
21/05/01 04:29:12 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
21/05/01 04:29:14 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
Spark context Web UI available at http://192.168.1.112:4040
Spark context available as 'sc' (master = local[*], app id = local-1619857737121).
Spark session available as 'spark'.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 2.1.1
      /_/

Using Scala version 2.11.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_212)
Type in expressions to have them evaluated.
Type :help for more information.

scala>

è®¿é—®ç½‘å€ï¼šhttp://192.168.1.112:4040
å¯ä»¥è®¿é—®å…¶å®¢æˆ·ç«¯



[chenbk@hadoop102 spark-local]$ mkdir input

åœ¨inputä¸‹åˆ›å»º2ä¸ªæ–‡ä»¶1.txtå’Œ2.txt

æ·»åŠ ä»¥ä¸‹å†…å®¹
hello chenbk
hello chenbk
hello hive
hello kafka

hello chenbk
hello spark

æ‰§è¡Œå‘½ä»¤ï¼ŒæŸ¥çœ‹ç»“æœ 
scala> sc.textFile("/opt/module/spark-local/input").flatMap(_.split(" ")).map((_,1)).reduceByKey(_+_).collect
res6: Array[(String, Int)] = Array((chenbk,2), (kafka,1), (hello,5), (spark,1), (hive,1))

scala>



é›†ç¾¤æ¨¡å¼ï¼ˆé‡ç‚¹ï¼‰
Standaloneæ¨¡å¼
Yarnæ¨¡å¼


1ã€Standaloneæ¨¡å¼
[chenbk@hadoop102 software]$ tar -zvxf spark-2.1.1-bin-hadoop2.7.tgz -C /opt/module/

é‡å‘½å
[chenbk@hadoop102 module]$ mv spark-2.1.1-bin-hadoop2.7 spark-standalone

è¿›å…¥
[chenbk@hadoop102 conf]$ pwd
/opt/module/spark-standalone/conf

é‡å‘½å
[chenbk@hadoop102 conf]$ mv slaves.template slaves
[chenbk@hadoop102 conf]$

ä¿®æ”¹slaveæ–‡ä»¶ï¼Œæ·»åŠ workèŠ‚ç‚¹ï¼š
[chenbk@hadoop102 conf]$ vim slaves

æ·»åŠ ä»¥ä¸‹å†…å®¹
hadoop102
hadoop103
hadoop104

# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

# A Spark Worker will be started on each of the machines listed below.
hadoop102
hadoop103
hadoop104


ä¿®æ”¹spark-env.shæ–‡ä»¶ï¼Œæ·»åŠ masterèŠ‚ç‚¹
é‡å‘½å
[chenbk@hadoop102 conf]$ mv spark-env.sh.template spark-env.sh
[chenbk@hadoop102 conf]$

ä¿®æ”¹
[chenbk@hadoop102 conf]$ vim spark-env.sh
[chenbk@hadoop102 conf]$

åœ¨æœ€åæ·»åŠ å¦‚ä¸‹å†…å®¹
# Generic options for the daemons used in the standalone deploy mode
# - SPARK_CONF_DIR      Alternate conf dir. (Default: ${SPARK_HOME}/conf)
# - SPARK_LOG_DIR       Where log files are stored.  (Default: ${SPARK_HOME}/logs)
# - SPARK_PID_DIR       Where the pid file is stored. (Default: /tmp)
# - SPARK_IDENT_STRING  A string representing this instance of spark. (Default: $USER)
# - SPARK_NICENESS      The scheduling priority for daemons. (Default: 0)
# - SPARK_NO_DAEMONIZE  Run the proposed command in the foreground. It will not output a PID file.
SPARK_MASTER_HOST=hadoop102
SPARK_MASTER_PORT=7077

åˆ†å‘spark-standaloneåŒ…
[chenbk@hadoop102 ~]$ sh xsync /opt/module/spark-standalone/

å¯åŠ¨é›†ç¾¤
[chenbk@hadoop102 spark-standalone]$ sbin/start-all.sh

starting org.apache.spark.deploy.master.Master, logging to /opt/module/spark-standalone/logs/spark-chenbk-org.apache.spark.deploy.master.Master-1-hadoop102.out
hadoop102: starting org.apache.spark.deploy.worker.Worker, logging to /opt/module/spark-standalone/logs/spark-chenbk-org.apache.spark.deploy.worker.Worker-1-hadoop102.out
hadoop103: starting org.apache.spark.deploy.worker.Worker, logging to /opt/module/spark-standalone/logs/spark-chenbk-org.apache.spark.deploy.worker.Worker-1-hadoop103.out
hadoop104: starting org.apache.spark.deploy.worker.Worker, logging to /opt/module/spark-standalone/logs/spark-chenbk-org.apache.spark.deploy.worker.Worker-1-hadoop104.out
[chenbk@hadoop102 spark-standalone]$

æŸ¥çœ‹å¯åŠ¨æƒ…å†µ

[chenbk@hadoop102 ~]$ sh jpsall
=====  hadoop102   =====
3824 Worker
2433 QuorumPeerMain
1922 DataNode
3250 SparkSubmit
2279 NodeManager
3703 Master
2875 Kafka
1790 NameNode
=====  hadoop103   =====
1953 QuorumPeerMain
2884 Worker
1462 ResourceManager
1580 NodeManager
1294 DataNode
=====  hadoop104   =====
2340 Kafka
1911 QuorumPeerMain
1768 NodeManager
2696 Worker
1561 DataNode
1631 SecondaryNameNode
[chenbk@hadoop102 ~]$


å½“ç½‘å€http://hadoop102:8080æ— æ³•è®¿é—®æ—¶
æŠ¥é”™åŸå› 
Sparkç«¯å£å·å’Œå…¶ä»–åº”ç”¨å‘ç”Ÿå†²çª

è§£å†³æ–¹æ³•
[chenbk@hadoop102 sbin]$ pwd
/opt/module/spark-standalone/sbin
[chenbk@hadoop102 sbin]$ vim start-master.sh

æŠŠ8080ä¿®æ”¹ä¸º8081
ORIGINAL_ARGS="$@"

. "${SPARK_HOME}/sbin/spark-config.sh"

. "${SPARK_HOME}/bin/load-spark-env.sh"

if [ "$SPARK_MASTER_PORT" = "" ]; then
  SPARK_MASTER_PORT=7077
fi

if [ "$SPARK_MASTER_HOST" = "" ]; then
  case `uname` in
      (SunOS)
          SPARK_MASTER_HOST="`/usr/sbin/check-hostname | awk '{print $NF}'`"
          ;;
      (*)
          SPARK_MASTER_HOST="`hostname -f`"
          ;;
  esac
fi

if [ "$SPARK_MASTER_WEBUI_PORT" = "" ]; then
  SPARK_MASTER_WEBUI_PORT=8081
fi

å…ˆå…³é—­sparké›†ç¾¤
[chenbk@hadoop102 spark-standalone]$ sbin/stop-all.sh
hadoop104: stopping org.apache.spark.deploy.worker.Worker
hadoop102: stopping org.apache.spark.deploy.worker.Worker
hadoop103: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
[chenbk@hadoop102 spark-standalone]$ jps
2433 QuorumPeerMain
1922 DataNode
3250 SparkSubmit
2279 NodeManager
4264 Jps
2875 Kafka
1790 NameNode

å†é‡å¯sparké›†ç¾¤
[chenbk@hadoop102 spark-standalone]$ sbin/start-all.sh

è¿›å…¥http://hadoop102:8081,å®Œæˆ

å°†çœ‹åˆ°å¦‚ä¸‹å†…å®¹

2.1.1 Spark Master at spark://hadoop102:7077
URL: spark://hadoop102:7077
REST URL: spark://hadoop102:6066 (cluster mode)
Alive Workers: 3
Cores in use: 16 Total, 0 Used
Memory in use: 4.7 GB Total, 0.0 B Used
Applications: 0 Running, 0 Completed
Drivers: 0 Running, 0 Completed
Status: ALIVE
Workers

Worker Id	Address	State	Cores	Memory
worker-20210501054113-192.168.1.112-35858	192.168.1.112:35858	ALIVE	8 (0 Used)	2.7 GB (0.0 B Used)
worker-20210501054114-192.168.1.113-43213	192.168.1.113:43213	ALIVE	4 (0 Used)	1024.0 MB (0.0 B Used)
worker-20210501054114-192.168.1.114-39093	192.168.1.114:39093	ALIVE	4 (0 Used)	1024.0 MB (0.0 B Used)

é…ç½®å†å²æœåŠ¡

ä¿®æ”¹åç§°
[chenbk@hadoop102 conf]$ mv spark-defaults.conf.template spark-defaults.conf
[chenbk@hadoop102 conf]$

æ‰§è¡Œè¯­å¥
[chenbk@hadoop102 conf]$ vim spark-defaults.conf

æ·»åŠ ä»¥ä¸‹å†…å®¹
# spark.executor.extraJavaOptions  -XX:+PrintGCDetails -Dkey=value -Dnumbers="one two three"
spark.eventLog.enabled          true
spark.eventLog.dir               hdfs://hadoop102:8020/directory

ç¾¤å‘æ–‡ä»¶
[chenbk@hadoop102 ~]$ sh xsync /opt/module/spark-standalone/conf/spark-defaults.conf
è¦åˆ†å‘çš„æ–‡ä»¶çš„è·¯å¾„æ˜¯:/opt/module/spark-standalone/conf/spark-defaults.conf
---------------------hadoop102---------------------
sending incremental file list

sent 56 bytes  received 12 bytes  136.00 bytes/sec
total size is 1,395  speedup is 20.51
---------------------hadoop103---------------------
sending incremental file list
spark-defaults.conf

sent 1,498 bytes  received 35 bytes  1,022.00 bytes/sec
total size is 1,395  speedup is 0.91
---------------------hadoop104---------------------
sending incremental file list
spark-defaults.conf

sent 1,498 bytes  received 35 bytes  3,066.00 bytes/sec
total size is 1,395  speedup is 0.91
[chenbk@hadoop102 ~]$

æ³¨æ„ï¼šéœ€è¦å¯åŠ¨Hadoopé›†ç¾¤ï¼ŒHDFSä¸Šçš„ç›®å½•éœ€è¦æå‰å­˜åœ¨ã€‚

ç›´æ¥åˆ›å»º
[chenbk@hadoop102 hadoop-3.1.3]$ hadoop fs -mkdir /directory
2021-05-01 05:58:19,936 INFO Configuration.deprecation: No unit for dfs.client.datanode-restart.timeout(30) assuming SECONDS
[chenbk@hadoop102 hadoop-3.1.3]$

ä¿®æ”¹spark-env.shæ–‡ä»¶ï¼Œæ·»åŠ å¦‚ä¸‹é…ç½®ï¼š
[chenbk@hadoop102 conf]$ vim spark-env.sh

æ·»åŠ ä»¥ä¸‹å†…å®¹
export SPARK_HISTORY_OPTS="
-Dspark.history.ui.port=18080 
-Dspark.history.fs.logDirectory=hdfs://hadoop102:8020/directory 
-Dspark.history.retainedApplications=30"


ç¾¤å‘æ–‡ä»¶
[chenbk@hadoop102 ~]$ sh xsync /opt/module/spark-standalone/conf/spark-env.sh

è¦åˆ†å‘çš„æ–‡ä»¶çš„è·¯å¾„æ˜¯:/opt/module/spark-standalone/conf/spark-env.sh
---------------------hadoop102---------------------
sending incremental file list

sent 49 bytes  received 12 bytes  122.00 bytes/sec
total size is 4,178  speedup is 68.49
---------------------hadoop103---------------------
sending incremental file list
spark-env.sh

sent 794 bytes  received 71 bytes  576.67 bytes/sec
total size is 4,178  speedup is 4.83
---------------------hadoop104---------------------
sending incremental file list
spark-env.sh

sent 794 bytes  received 71 bytes  576.67 bytes/sec
total size is 4,178  speedup is 4.83
[chenbk@hadoop102 ~]$

å¯åŠ¨å†å²æœåŠ¡
[chenbk@hadoop102 ~]$ cd /opt/module/spark-standalone/
[chenbk@hadoop102 spark-standalone]$ sbin/start-history-server.sh
starting org.apache.spark.deploy.history.HistoryServer, logging to /opt/module/spark-standalone/logs/spark-chenbk-org.apache.spark.deploy.history.HistoryServer-1-hadoop102.out
[chenbk@hadoop102 spark-standalone]$

å†æ¬¡æ‰§è¡Œä»»åŠ¡
[chenbk@hadoop102 spark-standalone]$ bin/spark-submit \
> --class org.apache.spark.examples.SparkPi \
> --master spark://hadoop102:7077 \
> --executor-memory 1G \
> --total-executor-cores 2 \
> ./examples/jars/spark-examples_2.11-2.1.1.jar \
>


åœ¨ç½‘ç«™ä¸ŠæŸ¥çœ‹ http://hadoop102:18080/

é…ç½®é«˜å¯ç”¨
åœæ­¢é›†ç¾¤
[chenbk@hadoop102 spark-standalone]$ sbin/stop-all.sh
hadoop104: stopping org.apache.spark.deploy.worker.Worker
hadoop102: stopping org.apache.spark.deploy.worker.Worker
hadoop103: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master

ä¿®æ”¹spark-env.shæ–‡ä»¶æ·»åŠ å¦‚ä¸‹é…ç½®ï¼š
[chenbk@hadoop102 conf]$ pwd
/opt/module/spark-standalone/conf

æ‰§è¡Œ
[chenbk@hadoop102 conf]$ vim spark-env.sh

æ³¨é‡Šæ‰å¦‚ä¸‹å†…å®¹ï¼š
#SPARK_MASTER_HOST=hadoop102
#SPARK_MASTER_PORT=7077

æ·»åŠ ä¸Šå¦‚ä¸‹å†…å®¹ã€‚é…ç½®ç”±Zookeeperç®¡ç†Masterï¼Œåœ¨ZookeeperèŠ‚ç‚¹ä¸­è‡ªåŠ¨åˆ›å»º/sparkç›®å½•ï¼Œç”¨äºç®¡ç†ï¼š

export SPARK_DAEMON_JAVA_OPTS="
-Dspark.deploy.recoveryMode=ZOOKEEPER 
-Dspark.deploy.zookeeper.url=hadoop102,hadoop103,hadoop104 
-Dspark.deploy.zookeeper.dir=/spark"

æ³¨æ„ï¼šZookeeper3.5çš„AdminServeré»˜è®¤ç«¯å£æ˜¯8080
æ‰€ä»¥å¯ä»¥
æ·»åŠ å¦‚ä¸‹ä»£ç 
Zookeeper3.5çš„AdminServeré»˜è®¤ç«¯å£æ˜¯8080ï¼Œå’ŒSparkçš„WebUIå†²çª
export SPARK_MASTER_WEBUI_PORT=8081

ç¾¤å‘æ–‡ä»¶

[chenbk@hadoop102 ~]$ sh xsync /opt/module/spark-standalone/conf/spark-env.sh
è¦åˆ†å‘çš„æ–‡ä»¶çš„è·¯å¾„æ˜¯:/opt/module/spark-standalone/conf/spark-env.sh
---------------------hadoop102---------------------
sending incremental file list

sent 49 bytes  received 12 bytes  40.67 bytes/sec
total size is 4,350  speedup is 71.31
---------------------hadoop103---------------------
sending incremental file list
spark-env.sh

sent 966 bytes  received 71 bytes  2,074.00 bytes/sec
total size is 4,350  speedup is 4.19
---------------------hadoop104---------------------
sending incremental file list
spark-env.sh

sent 966 bytes  received 71 bytes  691.33 bytes/sec
total size is 4,350  speedup is 4.19
[chenbk@hadoop102 ~]$


å¯åŠ¨é›†ç¾¤

[chenbk@hadoop102 spark-standalone]$ sbin/start-all.sh
starting org.apache.spark.deploy.master.Master, logging to /opt/module/spark-standalone/logs/spark-chenbk-org.apache.spark.deploy.master.Master-1-hadoop102.out
hadoop103: starting org.apache.spark.deploy.worker.Worker, logging to /opt/module/spark-standalone/logs/spark-chenbk-org.apache.spark.deploy.worker.Worker-1-hadoop103.out
hadoop102: starting org.apache.spark.deploy.worker.Worker, logging to /opt/module/spark-standalone/logs/spark-chenbk-org.apache.spark.deploy.worker.Worker-1-hadoop102.out
hadoop104: starting org.apache.spark.deploy.worker.Worker, logging to /opt/module/spark-standalone/logs/spark-chenbk-org.apache.spark.deploy.worker.Worker-1-hadoop104.out
[chenbk@hadoop102 spark-standalone]$ 

103ä¹Ÿå¯åŠ¨sbin/start-master.sh
[chenbk@hadoop103 spark-standalone]$ sbin/start-master.sh
starting org.apache.spark.deploy.master.Master, logging to /opt/module/spark-standalone/logs/spark-chenbk-org.apache.spark.deploy.master.Master-1-hadoop103.out
[chenbk@hadoop103 spark-standalone]$


æŸ¥çœ‹
[chenbk@hadoop102 ~]$ sh jpsall
=====  hadoop102   =====
2433 QuorumPeerMain
1922 DataNode
3250 SparkSubmit
5333 Master
5445 Worker
2279 NodeManager
2875 Kafka
4669 HistoryServer
1790 NameNode
=====  hadoop103   =====
1953 QuorumPeerMain
1462 ResourceManager
3305 Worker
3401 Master
1580 NodeManager
1294 DataNode
=====  hadoop104   =====
3250 Worker
2340 Kafka
1911 QuorumPeerMain
1768 NodeManager
1561 DataNode
1631 SecondaryNameNode
[chenbk@hadoop102 ~]$


åœ¨å¯åŠ¨ä¸€ä¸ªhadoop102çª—å£ï¼Œå°†/opt/module/spark-local/inputæ•°æ®ä¸Šä¼ åˆ°hadoopé›†ç¾¤çš„/inputç›®å½•
[chenbk@hadoop102 ~]$ hadoop fs -put /opt/module/spark-local/input/ /input
2021-05-01 06:29:49,868 INFO Configuration.deprecation: No unit for dfs.client.datanode-restart.timeout(30) assuming SECONDS
2021-05-01 06:29:51,124 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2021-05-01 06:29:51,631 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
[chenbk@hadoop102 ~]$


Spark HAé›†ç¾¤è®¿é—®
[chenbk@hadoop102 ~]$ cd /opt/module/spark-standalone/
[chenbk@hadoop102 spark-standalone]$ bin/spark-shell \
> --master spark://hadoop102:7077,hadoop103:7077 \
> --executor-memory 2g \
> --total-executor-cores 2

21/05/01 06:31:29 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
Spark context Web UI available at http://192.168.1.112:4041
Spark context available as 'sc' (master = spark://hadoop102:7077,hadoop103:7077, app id = app-20210501063102-0000).
Spark session available as 'spark'.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 2.1.1
      /_/

Using Scala version 2.11.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_212)
Type in expressions to have them evaluated.
Type :help for more information.

scala>


æ‰§è¡ŒWordCountç¨‹åº
scala> sc.textFile("hdfs://hadoop102:8020/input").flatMap(_.split(" ")).map((_,1)).reduceByKey(_+_).collect
res0: Array[(String, Int)] = Array((chenbk,2), (kafka,1), (hello,5), (spark,1), (hive,1))

scala>


é«˜å¯ç”¨æµ‹è¯•

Killæ‰hadoop102çš„masterè¿›ç¨‹ï¼Œé¡µé¢ä¸­è§‚å¯Ÿhttp://hadoop103:8080/çš„çŠ¶æ€æ˜¯å¦åˆ‡æ¢ä¸ºactiveã€‚

[chenbk@hadoop102 spark-standalone]$ jps
2433 QuorumPeerMain
1922 DataNode
3250 SparkSubmit
5333 Master
5445 Worker
6117 Jps
2279 NodeManager
2875 Kafka
4669 HistoryServer
1790 NameNode
[chenbk@hadoop102 spark-standalone]$ kill -9 5333
[chenbk@hadoop102 spark-standalone]$


102å·²åœç”¨ï¼Œ103è¿è¡Œï¼Œæµ‹è¯•æˆåŠŸ

å†å¯åŠ¨hadoop102çš„masterè¿›ç¨‹
[chenbk@hadoop102 spark-standalone]$ sbin/start-master.sh
starting org.apache.spark.deploy.master.Master, logging to /opt/module/spark-standalone/logs/spark-chenbk-org.apache.spark.deploy.master.Master-1-hadoop102.out
[chenbk@hadoop102 spark-standalone]$






Linux å®‰è£…oracle
å…ˆä¸Šä¼ æ–‡ä»¶åˆ°æœåŠ¡å™¨ä¸­
[root@localhost ~]# ll
æ€»ç”¨é‡ 2300456
-rw-------. 1 root root       1335 5æœˆ   1 07:02 anaconda-ks.cfg
-rw-r--r--. 1 root root    4976069 5æœˆ   1 11:48 CentOS7Oracle11gInstallHelper.zip
-rw-r--r--. 1 root root 1239269270 5æœˆ   1 11:44 linux.x64_11gR2_database_1of2.zip
-rw-r--r--. 1 root root 1111416131 5æœˆ   1 11:44 linux.x64_11gR2_database_2of2.zip
[root@localhost ~]#

å…ˆä¸‹è½½unzipå·¥å…·
[root@localhost ~]# yum install unzip
äº‹åŠ¡æ¦‚è¦
======================================================================================================================
å®‰è£…  1 è½¯ä»¶åŒ…

æ€»ä¸‹è½½é‡ï¼š171 k
å®‰è£…å¤§å°ï¼š365 k
Is this ok [y/d/N]: y

unzip-6.0-21.el7.x86_64.rpm çš„å…¬é’¥å°šæœªå®‰è£…
unzip-6.0-21.el7.x86_64.rpm                                                                    | 171 kB  00:00:00
ä» file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7 æ£€ç´¢å¯†é’¥
å¯¼å…¥ GPG key 0xF4A80EB5:
 ç”¨æˆ·ID     : "CentOS-7 Key (CentOS 7 Official Signing Key) <security@centos.org>"
 æŒ‡çº¹       : 6341 ab27 53d7 8a78 a7c2 7bb1 24c6 a8a7 f4a8 0eb5
 è½¯ä»¶åŒ…     : centos-release-7-9.2009.0.el7.centos.x86_64 (@anaconda)
 æ¥è‡ª       : /etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7
æ˜¯å¦ç»§ç»­ï¼Ÿ[y/N]ï¼šy
å·²å®‰è£…:
  unzip.x86_64 0:6.0-21.el7

å®Œæ¯•ï¼
[root@localhost ~]#

è§£å‹
[root@localhost ~]# unzip CentOS7Oracle11gInstallHelper.zip

root@localhost ~]# unzip CentOS7Oracle11gInstallHelper.zip
Archive:  CentOS7Oracle11gInstallHelper.zip
   creating: CentOS7Oracle11gInstallHelper/
  inflating: CentOS7Oracle11gInstallHelper/.DS_Store
  inflating: __MACOSX/CentOS7Oracle11gInstallHelper/._.DS_Store
  inflating: CentOS7Oracle11gInstallHelper/helper.sh
  inflating: CentOS7Oracle11gInstallHelper/zysong.ttf
  inflating: CentOS7Oracle11gInstallHelper/pdksh-5.2.14-37.el5_8.1.x86_64.rpm



æŸ¥çœ‹
[root@localhost ~]# ll
æ€»ç”¨é‡ 2300456
-rw-------. 1 root root       1335 5æœˆ   1 07:02 anaconda-ks.cfg
drwxr-xr-x. 2 root root        100 2æœˆ   2 20:45 CentOS7Oracle11gInstallHelper
-rw-r--r--. 1 root root    4976069 5æœˆ   1 11:48 CentOS7Oracle11gInstallHelper.zip
-rw-r--r--. 1 root root 1239269270 5æœˆ   1 11:44 linux.x64_11gR2_database_1of2.zip
-rw-r--r--. 1 root root 1111416131 5æœˆ   1 11:44 linux.x64_11gR2_database_2of2.zip
drwxr-xr-x. 3 root root         43 5æœˆ   1 11:53 __MACOSX
[root@localhost ~]#

ç»™æƒé™
[root@localhost ~]# cd CentOS7Oracle11gInstallHelper
[root@localhost CentOS7Oracle11gInstallHelper]# chmod +x helper.sh

æ‰§è¡Œ
[root@localhost CentOS7Oracle11gInstallHelper]# ./helper.sh


ä½œä¸ºä¾èµ–è¢«å®‰è£…:
  cpp.x86_64 0:4.8.5-44.el7                            glibc-headers.x86_64 0:2.17-324.el7_9
  kernel-headers.x86_64 0:3.10.0-1160.25.1.el7         libmpc.x86_64 0:1.0.1-3.el7
  libtool-ltdl.i686 0:2.4.2-22.el7_3                   lm_sensors-libs.x86_64 0:3.4.0-8.20160601gitf9185e5.el7
  mpfr.x86_64 0:3.1.1-4.el7                            ncurses-libs.i686 0:5.9-14.20130511.el7_4
  nss-softokn-freebl.i686 0:3.53.1-6.el7_9             readline.i686 0:6.2-11.el7
  zlib-devel.x86_64 0:1.2.7-19.el7_9

æ›´æ–°å®Œæ¯•:
  glibc.x86_64 0:2.17-324.el7_9

ä½œä¸ºä¾èµ–è¢«å‡çº§:
  glibc-common.x86_64 0:2.17-324.el7_9   nspr.x86_64 0:4.25.0-2.el7_9   nss-softokn-freebl.x86_64 0:3.53.1-6.el7_9
  nss-util.x86_64 0:3.53.1-1.el7_9       zlib.x86_64 0:1.2.7-19.el7_9

å®Œæ¯•ï¼
-------------æ­£åœ¨å®‰è£…ä¸­æ˜“å®‹ä½“18030-------------
å­—ä½“å®‰è£…å®Œæˆï¼
zysong.ttf
-------------æ­£åœ¨å°è¯•å®‰è£…pdksh-5.2.14-------------
å°è¯•å¸è½½å†²çªksh-20120801-142.el7.x86_64
è­¦å‘Šï¼špdksh-5.2.14-37.el5_8.1.x86_64.rpm: å¤´V3 DSA/SHA1 Signature, å¯†é’¥ ID e8562897: NOKEY
å‡†å¤‡ä¸­...                          ################################# [100%]
æ­£åœ¨å‡çº§/å®‰è£…...
   1:pdksh-5.2.14-37.el5_8.1          ################################# [100%]
æŸ¥è¯¢æ˜¯å¦å®‰è£…æˆåŠŸï¼š
pdksh-5.2.14-37.el5_8.1.x86_64
è„šæœ¬æ‰§è¡Œå®Œæˆï¼äº«å—oracleå®‰è£…å§!
 

å·²å®Œæˆä¾èµ–å®‰è£…

åˆ›å»ºç”¨æˆ·&å¼€å¯VNCæœåŠ¡
#åˆ›å»ºdatabaseç”¨æˆ·ç»„
#åˆ›å»ºoracleç”¨æˆ·å¹¶æ”¾å…¥databaseç»„ä¸­
#é¦–æ¬¡è¿è¡Œï¼Œç”Ÿæˆ~/.vnc/xstartupç­‰é…ç½®æ–‡ä»¶
[root@localhost CentOS7Oracle11gInstallHelper]# groupadd database
[root@localhost CentOS7Oracle11gInstallHelper]# useradd oracle -g database
[root@localhost CentOS7Oracle11gInstallHelper]# su oracle
[oracle@localhost CentOS7Oracle11gInstallHelper]$ vncserver :1 -geometry 1024x768

[oracle@localhost CentOS7Oracle11gInstallHelper]$ vncserver :1 -geometry 1024x768

You will require a password to access your desktops.

Password:
Verify:
Would you like to enter a view-only password (y/n)? n
A view-only password is not used
xauth:  file /home/oracle/.Xauthority does not exist

New 'localhost.localdomain:1 (oracle)' desktop is localhost.localdomain:1

Creating default startup script /home/oracle/.vnc/xstartup
Creating default config /home/oracle/.vnc/config
Starting applications specified in /home/oracle/.vnc/xstartup
Log file is /home/oracle/.vnc/localhost.localdomain:1.log



Password:
Verify:
å¯†ç éƒ½ä¸ºï¼švnc@2021


å®¢æˆ·ç«¯è¿æ¥VNCå®ç°è¿œç¨‹æ§åˆ¶

å…ˆæ‰§è¡Œ
#é…ç½®VNCé»˜è®¤å¯åŠ¨openbox
[oracle@localhost CentOS7Oracle11gInstallHelper]$ echo "openbox-session &" > ~/.vnc/xstartup
# åœæ­¢æœåŠ¡
[oracle@localhost CentOS7Oracle11gInstallHelper]$ vncserver -kill :1
#é‡æ–°å¼€å¯vncæœåŠ¡
[oracle@localhost CentOS7Oracle11gInstallHelper]$ vncserver :1 -geometry 1024x768
[oracle@localhost CentOS7Oracle11gInstallHelper]$ echo "openbox-session &" > ~/.vnc/xstartup
[oracle@localhost CentOS7Oracle11gInstallHelper]$ vncserver -kill :1
Killing Xvnc process ID 16824
[oracle@localhost CentOS7Oracle11gInstallHelper]$ vncserver :1 -geometry 1024x768

New 'localhost.localdomain:1 (oracle)' desktop is localhost.localdomain:1

Starting applications specified in /home/oracle/.vnc/xstartup
Log file is /home/oracle/.vnc/localhost.localdomain:1.log



è¾“å…¥192.168.1.121:5901 è¿æ¥æˆåŠŸ

å¦‚æœæŠ¥é”™ï¼ˆæ²¡æœ‰å›¾å½¢æ˜¾ç¤ºï¼‰

å…ˆå¯åŠ¨vnc

[oracle@localhost CentOS7Oracle11gInstallHelper]$ vncserver
New 'localhost.localdomain:2 (oracle)' desktop is localhost.localdomain:2

Starting applications specified in /home/oracle/.vnc/xstartup
Log file is /home/oracle/.vnc/localhost.localdomain:2.log

æŸ¥çœ‹åˆ° localhost.localdomain:2

æ‰§è¡Œexport DISPLAY=localhost.localdomain:2å‘½ä»¤
[oracle@localhost CentOS7Oracle11gInstallHelper]$  export DISPLAY=localhost.localdomain:2
å†æ‰§è¡Œxhost +
[oracle@localhost CentOS7Oracle11gInstallHelper]$ xhost +
access control disabled, clients can connect from any host

é‡æ–°è¿æ¥

ä¸Šä¼ å¹¶è§£å‹å®‰è£…åŒ…
[root@localhost ~]# mv linux.x64_11gR2_database_1of2.zip /home/oracle/
[root@localhost ~]# mv linux.x64_11gR2_database_2of2.zip /home/oracle/
[root@localhost ~]# su oracle
[oracle@localhost root]$ cd /home/oracle/
[oracle@localhost ~]$ ll
æ€»ç”¨é‡ 2295592
-rw-r--r--. 1 root root 1239269270 5æœˆ   1 11:44 linux.x64_11gR2_database_1of2.zip
-rw-r--r--. 1 root root 1111416131 5æœˆ   1 11:44 linux.x64_11gR2_database_2of2.zip
[oracle@localhost ~]$

æ‰§è¡Œè§£å‹å‘½ä»¤

[oracle@localhost ~]$ unzip linux.x64_11gR2_database_1of2.zip
[oracle@localhost ~]$ unzip linux.x64_11gR2_database_2of2.zip

æŸ¥çœ‹
[oracle@localhost ~]$ ll
æ€»ç”¨é‡ 2295592
drwxr-xr-x. 8 oracle database        128 8æœˆ  20 2009 database
-rw-r--r--. 1 root   root     1239269270 5æœˆ   1 11:44 linux.x64_11gR2_database_1of2.zip
-rw-r--r--. 1 root   root     1111416131 5æœˆ   1 11:44 linux.x64_11gR2_database_2of2.zip
[oracle@localhost ~]$


å®‰è£…oracleå®æˆ˜

oracleç”¨æˆ·ç™»å½•vncè¿œç¨‹æ¡Œé¢ã€‚
#è¿›å…¥å®‰è£…ç›®å½•
cd ~/database/
#è¿è¡Œå®‰è£…ç¨‹åº
./runInstaller

[oracle@localhost ~]$ cd database/
[oracle@localhost database]$ ./runInstaller 
\u6b63\u5728\u542f\u52a8 Oracle Universal Installer...


ä¸‹è½½è½¯ä»¶æ›´æ–°

æ ¹æ®ä¸ªäººéœ€è¦é€‰æ‹©ï¼Œæˆ‘è¿™é‡Œé€‰æ‹© è·³è¿‡è½¯ä»¶æ›´æ–°ï¼ˆSï¼‰ã€‚

 orcl å¯†ç ä¸ºï¼šChenbk111


å¦‚æœæ‰§è¡Œä¿®è¡¥å¹¶å†æ¬¡æ£€æŸ¥ï¼ˆFï¼‰æœ‰æç¤ºï¼š
æ‰§è¡Œä¸‹é¢è¯­å¥
[root@localhost oracle]# sh /tmp/CVU_11.2.0.1.0_oracle/runfixup.sh

[root@localhost oracle]# sh /tmp/CVU_11.2.0.1.0_oracle/runfixup.sh
Response file being used is :/tmp/CVU_11.2.0.1.0_oracle/fixup.response
Enable file being used is :/tmp/CVU_11.2.0.1.0_oracle/fixup.enable
Log file location: /tmp/CVU_11.2.0.1.0_oracle/orarun.log
Setting Kernel Parameters...
/tmp/CVU_11.2.0.1.0_oracle/orarun.sh: ç¬¬ 244 è¡Œ:[: 18446744073692774399: æœŸå¾…æ•´æ•°è¡¨è¾¾å¼
The value for shmmax in response file is not greater than value of shmmax for current session. Hence not changing it.
/tmp/CVU_11.2.0.1.0_oracle/orarun.sh: ç¬¬ 335 è¡Œ:[: 18446744073692774399: æœŸå¾…æ•´æ•°è¡¨è¾¾å¼
The value for shmall in response file is not greater than value of shmall for current session. Hence not changing it.
The value for semmni in response file is not greater than value of semmni for current session. Hence not changing it.
kernel.sem = 250 32000 100 128
fs.file-max = 6815744
net.ipv4.ip_local_port_range = 9000 65500
net.core.rmem_default = 262144
net.core.wmem_default = 262144
net.core.rmem_max = 4194304
net.core.wmem_max = 1048576
fs.aio-max-nr = 1048576
uid=1000(oracle) gid=1000(database) ç»„=1000(database)
[root@localhost oracle]#

æ‰§è¡Œé…ç½®è„šæœ¬

[root@localhost ~]# sh  /home/oracle/app/oraInventory/orainstRoot.sh
æ›´æ”¹æƒé™/home/oracle/app/oraInventory.
æ·»åŠ ç»„çš„è¯»å–å’Œå†™å…¥æƒé™ã€‚
åˆ é™¤å…¨å±€çš„è¯»å–, å†™å…¥å’Œæ‰§è¡Œæƒé™ã€‚

æ›´æ”¹ç»„å/home/oracle/app/oraInventory åˆ° database.
è„šæœ¬çš„æ‰§è¡Œå·²å®Œæˆã€‚
[root@localhost ~]#


[root@localhost ~]# sh /home/oracle/app/oracle/product/11.2.0/dbhome_1/root.sh
Running Oracle 11g root.sh script...

The following environment variables are set as:
    ORACLE_OWNER= oracle
    ORACLE_HOME=  /home/oracle/app/oracle/product/11.2.0/dbhome_1

Enter the full pathname of the local bin directory: [/usr/local/bin]: bin
Creating bin directory...
   Copying dbhome to bin ...
   Copying oraenv to bin ...
   Copying coraenv to bin ...


Creating /etc/oratab file...
Entries will be added to the /etc/oratab file as needed by
Database Configuration Assistant when a database is created
Finished running generic part of root.sh script.
Now product-specific root actions will be performed.
Finished product-specific root actions.
[root@localhost ~]#



é…ç½®ç¯å¢ƒå˜é‡

[root@localhost ~]# su oracle
[oracle@localhost root]$ vi ~/.bash_profile
[oracle@localhost root]$


æ·»åŠ ä»¥ä¸‹å†…å®¹
export ORACLE_HOME=/home/oracle/app/oracle/product/11.2.0/dbhome_1/
export ORACLE_SID=orcl
export PATH=$PATH:$ORACLE_HOME/bin

ä½¿ç”¨é…ç½®æ–‡ä»¶ç«‹å³ç”Ÿæ•ˆã€‚
[oracle@localhost root]$ source ~/.bash_profile
[oracle@localhost root]$


å¯åŠ¨oracle

[oracle@localhost root]$ sqlplus /nolog

SQL*Plus: Release 11.2.0.1.0 Production on Sat May 1 13:38:27 2021

Copyright (c) 1982, 2009, Oracle.  All rights reserved.

SQL>

åˆ›å»ºç”¨æˆ·
SQL> connect as sysdba
Enter user-name: sys
Enter password:
Connected to an idle instance.
SQL>

SQL> select 1 from dual;
select 1 from dual
*
ERROR at line 1:
ORA-01034: ORACLE not available
Process ID: 0
Session ID: 0 Serial number: 0


SQL>

æ²¡æœ‰é—®é¢˜ï¼Œè¯´æ˜oracleæœ¬åœ°è¿æ¥oracleæˆåŠŸã€‚


å¯åŠ¨ç›‘å¬
é‡å¯æœåŠ¡å™¨åï¼Œè¿›å…¥oracle
[root@localhost oracle]# su - oracle
ä¸Šä¸€æ¬¡ç™»å½•ï¼šå…­ 5æœˆ  1 13:48:03 EDT 2021pts/0 ä¸Š

æ³¨æ„äº‹é¡¹
ç¬¬ä¸€ï¼šä¿®æ”¹ä¸»æœºåç§°ï¼šoracle
ç¬¬äºŒï¼šä¿®æ”¹hostsæ–‡ä»¶ï¼Œåœ¨æ–‡ä»¶ä¸­æ·»åŠ ï¼š192.168.1.121 oracle
ç¬¬ä¸‰ï¼šä¿®æ”¹listener.ora
[oracle@orcale admin]$ vim listener.ora

ä¿®æ”¹ä»¥ä¸‹æ–‡ä»¶ï¼š
# listener.ora Network Configuration File: /home/oracle/app/oracle/product/11.2.0/dbhome_1/network/admin/listener.ora
# Generated by Oracle configuration tools.
LISTENER =
  (DESCRIPTION_LIST =
    (DESCRIPTION =
      (ADDRESS = (PROTOCOL = TCP)(HOST = oracle)(PORT = 1521))
      (ADDRESS = (PROTOCOL = IPC)(KEY = EXTPROC1521))
    )
  )
ADR_BASE_LISTENER = /home/oracle/app/oracle

ç¬¬å››ï¼štnsnames.ora
[oracle@orcale admin]$ vim tnsnames.ora

ä¿®æ”¹ä»¥ä¸‹æ–‡ä»¶
# tnsnames.ora Network Configuration File: /home/oracle/app/oracle/product/11.2.0/dbhome_1/network/admin/tnsnames.ora
# Generated by Oracle configuration tools.

ORCL =
  (DESCRIPTION =
    (ADDRESS = (PROTOCOL = TCP)(HOST = oracle)(PORT = 1521))
    (CONNECT_DATA =
      (SERVER = DEDICATED)
      (SERVICE_NAME = orcl)
    )
  )

ç¬¬äº”ï¼šå¯åŠ¨ç›‘å¬
[oracle@orcale admin]$ lsnrctl start

LSNRCTL for Linux: Version 11.2.0.1.0 - Production on 02-MAY-2021 02:08:31

Copyright (c) 1991, 2009, Oracle.  All rights reserved.

Starting /home/oracle/app/oracle/product/11.2.0/dbhome_1//bin/tnslsnr: please wait...

TNSLSNR for Linux: Version 11.2.0.1.0 - Production
System parameter file is /home/oracle/app/oracle/product/11.2.0/dbhome_1/network/admin/listener.ora
Log messages written to /home/oracle/app/oracle/diag/tnslsnr/orcale/listener/alert/log.xml
Listening on: (DESCRIPTION=(ADDRESS=(PROTOCOL=tcp)(HOST=orcale)(PORT=1521)))
Listening on: (DESCRIPTION=(ADDRESS=(PROTOCOL=ipc)(KEY=EXTPROC1521)))

Connecting to (DESCRIPTION=(ADDRESS=(PROTOCOL=TCP)(HOST=oracle)(PORT=1521)))
STATUS of the LISTENER
------------------------
Alias                     LISTENER
Version                   TNSLSNR for Linux: Version 11.2.0.1.0 - Production
Start Date                02-MAY-2021 02:08:41
Uptime                    0 days 0 hr. 0 min. 35 sec
Trace Level               off
Security                  ON: Local OS Authentication
SNMP                      OFF
Listener Parameter File   /home/oracle/app/oracle/product/11.2.0/dbhome_1/network/admin/listener.ora
Listener Log File         /home/oracle/app/oracle/diag/tnslsnr/orcale/listener/alert/log.xml
Listening Endpoints Summary...
  (DESCRIPTION=(ADDRESS=(PROTOCOL=tcp)(HOST=orcale)(PORT=1521)))
  (DESCRIPTION=(ADDRESS=(PROTOCOL=ipc)(KEY=EXTPROC1521)))
The listener supports no services
The command completed successfully

ç¬¬å…­ï¼šç”¨sqlplus / as sysdbaè¿›å…¥æ•°æ®åº“
[oracle@orcale admin]$ sqlplus / as sysdba

SQL*Plus: Release 11.2.0.1.0 Production on Sun May 2 02:10:27 2021

Copyright (c) 1982, 2009, Oracle.  All rights reserved.

Connected to an idle instance.

SQL> 

å¦‚æœå‡ºç°ï¼Œä»¥ä¸‹é—®é¢˜ï¼Œå°±æ˜¯æ•°æ®åº“æ²¡æœ‰å¯åŠ¨

SQL> show parameter service_names
ORA-01034: ORACLE not available
Process ID: 0
Session ID: 0 Serial number: 0

ç¬¬ä¸ƒï¼šå¯åŠ¨æ•°æ®åº“
SQL> startup
ORACLE instance started.

Total System Global Area  768294912 bytes
Fixed Size		    2217304 bytes
Variable Size		  473959080 bytes
Database Buffers	  285212672 bytes
Redo Buffers		    6905856 bytes
Database mounted.
Database opened.


æµ‹è¯•ï¼š

SQL> show parameter service_names

NAME				     TYPE	 VALUE
------------------------------------ ----------- ------------------------------
service_names


SQL> select 1 from dual;

	 1
----------
	 1

SQL>




é—®é¢˜æ±‡æ€»

1ã€ç›‘å¬èµ·ä¸æ¥
LSNRCTL for Linux: Version 11.2.0.1.0 - Production on 01-MAY-2021 13:51:21

Copyright (c) 1991, 2009, Oracle.  All rights reserved.

Connecting to (DESCRIPTION=(ADDRESS=(PROTOCOL=IPC)(KEY=EXTPROC1521)))
TNS-12541: TNS:no listener
 TNS-12560: TNS:protocol adapter error
  TNS-00511: No listener
   Linux Error: 111: Connection refused
Connecting to (DESCRIPTION=(ADDRESS=(PROTOCOL=TCP)(HOST=localhost)(PORT=1521)))
TNS-12541: TNS:no listener
 TNS-12560: TNS:protocol adapter error
  TNS-00511: No listener
   Linux Error: 111: Connection refused
é…ç½®æœ‰é—®é¢˜
ä¿®æ”¹ï¼šlistener.oraåŠtnsnames.ora


     
é—®é¢˜2
[oracle@localhost ~]$ sqlplus /nolog

SQL*Plus: Release 11.2.0.1.0 Production on Sat May 1 13:51:39 2021

Copyright (c) 1982, 2009, Oracle.  All rights reserved.

SQL> connect / as sysdba
Connected to an idle instance.
SQL> startup

æ•°æ®åº“æ²¡æœ‰èµ·æ¥


è½¯ä»¶è¿æ¥ä¸æˆåŠŸ
æ‰§è¡Œä»¥ä¸‹å‘½ä»¤
[oracle@localhost admin]$ cat tnsnames.ora
# tnsnames.ora Network Configuration File: /home/oracle/app/oracle/product/11.2.0/dbhome_1/network/admin/tnsnames.ora
# Generated by Oracle configuration tools.

ORCL =
  (DESCRIPTION =
    (ADDRESS = (PROTOCOL = TCP)(HOST = localhost)(PORT = 1521))
    (CONNECT_DATA =
      (SERVER = DEDICATED)
      (SERVICE_NAME = orcl)
    )
  )

[oracle@localhost admin]$ vi tnsnames.ora


ä¿®æ”¹ä»¥ä¸‹å†…å®¹

ORCL =
  (DESCRIPTION =
    (ADDRESS = (PROTOCOL = TCP)(HOST = orcale)(PORT = 1521))
    (CONNECT_DATA =
      (SERVER = DEDICATED)
      (SERVICE_NAME = orcl)
    )
  )

ä¿®æ”¹ä¸»æœºåï¼š
[root@localhost oracle]# vi /etc/hostname
orcale


åœ¨é…ç½®ç›‘å¬æ—¶ï¼Œéœ€è¦æŠŠæ ¼å¼å¯¹å¥½
æ ¼å¼å‡ºäº†é—®é¢˜
[oracle@orcale admin]$ lsnrctl start

LSNRCTL for Linux: Version 11.2.0.1.0 - Production on 01-MAY-2021 23:58:03

Copyright (c) 1991, 2009, Oracle.  All rights reserved.

Starting /home/oracle/app/oracle/product/11.2.0/dbhome_1//bin/tnslsnr: please wait...

TNSLSNR for Linux: Version 11.2.0.1.0 - Production
System parameter file is /home/oracle/app/oracle/product/11.2.0/dbhome_1/network/admin/listener.ora
Log messages written to /home/oracle/app/oracle/product/11.2.0/dbhome_1/log/diag/tnslsnr/orcale/listener/alert/log.xml
TNS-01150: The address of the specified listener name is incorrect

Listener failed to start. See the error message(s) above...



è½¯ä»¶è¿æ¥å‡ºç°ï¼Œä»¥ä¸‹é”™è¯¯


IO é”™è¯¯: The Network Adapter could not establish the connection
  The Network Adapter could not establish the connection
  The Network Adapter could not establish the connection
    Connection refused, socket connect lapse 0 ms. /192.168.1.121 1521 0 1 true
    Connection refused, socket connect lapse 0 ms. /192.168.1.121 1521 0 1 true
      Connection refused
      Connection refused
è¿™æ˜¯ç›‘å¬æ²¡æœ‰èµ·æ¥

å¯åŠ¨ç›‘å¬
[oracle@orcale ~]$ lsnrctl start

LSNRCTL for Linux: Version 11.2.0.1.0 - Production on 02-MAY-2021 02:39:51

Copyright (c) 1991, 2009, Oracle.  All rights reserved.

Starting /home/oracle/app/oracle/product/11.2.0/dbhome_1//bin/tnslsnr: please wait...

TNSLSNR for Linux: Version 11.2.0.1.0 - Production
System parameter file is /home/oracle/app/oracle/product/11.2.0/dbhome_1/network/admin/listener.ora
Log messages written to /home/oracle/app/oracle/diag/tnslsnr/orcale/listener/alert/log.xml
Listening on: (DESCRIPTION=(ADDRESS=(PROTOCOL=tcp)(HOST=orcale)(PORT=1521)))
Listening on: (DESCRIPTION=(ADDRESS=(PROTOCOL=ipc)(KEY=EXTPROC1521)))

Connecting to (DESCRIPTION=(ADDRESS=(PROTOCOL=TCP)(HOST=oracle)(PORT=1521)))
STATUS of the LISTENER
------------------------
Alias                     LISTENER
Version                   TNSLSNR for Linux: Version 11.2.0.1.0 - Production
Start Date                02-MAY-2021 02:40:07
Uptime                    0 days 0 hr. 0 min. 55 sec
Trace Level               off
Security                  ON: Local OS Authentication
SNMP                      OFF
Listener Parameter File   /home/oracle/app/oracle/product/11.2.0/dbhome_1/network/admin/listener.ora
Listener Log File         /home/oracle/app/oracle/diag/tnslsnr/orcale/listener/alert/log.xml
Listening Endpoints Summary...
  (DESCRIPTION=(ADDRESS=(PROTOCOL=tcp)(HOST=orcale)(PORT=1521)))
  (DESCRIPTION=(ADDRESS=(PROTOCOL=ipc)(KEY=EXTPROC1521)))
The listener supports no services
The command completed successfully
[oracle@orcale ~]$


å¦‚æœæ˜¯ä»¥ä¸‹é”™è¯¯

Listener refused the connection with the following error:
ORA-12514, TNS:listener does not currently know of service requested in connect descriptor
 
Listener refused the connection with the following error:
ORA-12514, TNS:listener does not currently know of service requested in connect descriptor
 
æ•°æ®åº“æ²¡æœ‰å¯åŠ¨

å…ˆå¯åŠ¨æ•°æ®åº“

[oracle@orcale ~]$ sqlplus / as sysdba

SQL*Plus: Release 11.2.0.1.0 Production on Sun May 2 02:42:46 2021

Copyright (c) 1982, 2009, Oracle.  All rights reserved.

Connected to an idle instance.

SQL> startup
ORACLE instance started.

Total System Global Area  768294912 bytes
Fixed Size		    2217304 bytes
Variable Size		  473959080 bytes
Database Buffers	  285212672 bytes
Redo Buffers		    6905856 bytes
Database mounted.
Database opened.
SQL> select 1 from dual;

	 1
----------
	 1

SQL>

æŸ¥çœ‹ ç›‘å¬åŠ¨æ€
lsnrctl status

è‡ªåŠ¨è„šæœ¬å¯åŠ¨

ä¿®æ”¹ä»¥ä¸‹å†…å®¹
[oracle@orcale ~]$ vim /etc/oratab

# This file is used by ORACLE utilities.  It is created by root.sh
# and updated by the Database Configuration Assistant when creating
# a database.

# A colon, ':', is used as the field terminator.  A new line terminates
# the entry.  Lines beginning with a pound sign, '#', are comments.
#
# Entries are of the form:
#   $ORACLE_SID:$ORACLE_HOME:<N|Y>:
#
# The first and second fields are the system identifier and home
# directory of the database respectively.  The third filed indicates
# to the dbstart utility that the database should , "Y", or should not,
# "N", be brought up at system boot time.
#
# Multiple entries with the same $ORACLE_SID are not allowed.
#
#
orcl:/home/oracle/app/oracle/product/11.2.0/dbhome_1:Y

åœ¨ /etc/init.d/ ä¸‹åˆ›å»ºæ–‡ä»¶oracleï¼Œå†…å®¹å¦‚ä¸‹ï¼š

#!/bin/sh
# chkconfig: 35 80 10
# description: Oracle auto start-stop script.

#
# Set ORA_HOME to be equivalent to the $ORACLE_HOME
# from which you wish to execute dbstart and dbshut;
#
# Set ORA_OWNER to the user id of the owner of the
# Oracle database in ORA_HOME.
ORA_HOME=/home/oracle/app/oracle/product/11.2.0/dbhome_1
ORA_OWNER=oracle
if [ ! -f $ORA_HOME/bin/dbstart ]
then
    echo "Oracle startup: cannot start"
    exit
fi
case "$1" in
'start')
# Start the Oracle databases:
echo "Starting Oracle Databases ... "
echo "-------------------------------------------------" >> /var/log/oracle
date +" %T %a %D : Starting Oracle Databases as part of system up." >> /var/log/oracle
echo "-------------------------------------------------" >> /var/log/oracle
su - $ORA_OWNER -c "$ORA_HOME/bin/dbstart" >>/var/log/oracle
echo "Done"

# Start the Listener:
echo "Starting Oracle Listeners ... "
echo "-------------------------------------------------" >> /var/log/oracle
date +" %T %a %D : Starting Oracle Listeners as part of system up." >> /var/log/oracle
echo "-------------------------------------------------" >> /var/log/oracle
su - $ORA_OWNER -c "$ORA_HOME/bin/lsnrctl start" >>/var/log/oracle
echo "Done."
echo "-------------------------------------------------" >> /var/log/oracle
date +" %T %a %D : Finished." >> /var/log/oracle
echo "-------------------------------------------------" >> /var/log/oracle
touch /var/lock/subsys/oracle
;;

'stop')
# Stop the Oracle Listener:
echo "Stoping Oracle Listeners ... "
echo "-------------------------------------------------" >> /var/log/oracle
date +" %T %a %D : Stoping Oracle Listener as part of system down." >> /var/log/oracle
echo "-------------------------------------------------" >> /var/log/oracle
su - $ORA_OWNER -c "$ORA_HOME/bin/lsnrctl stop" >>/var/log/oracle
echo "Done."
rm -f /var/lock/subsys/oracle

# Stop the Oracle Database:
echo "Stoping Oracle Databases ... "
echo "-------------------------------------------------" >> /var/log/oracle
date +" %T %a %D : Stoping Oracle Databases as part of system down." >> /var/log/oracle
echo "-------------------------------------------------" >> /var/log/oracle
su - $ORA_OWNER -c "$ORA_HOME/bin/dbshut" >>/var/log/oracle
echo "Done."
echo ""
echo "-------------------------------------------------" >> /var/log/oracle
date +" %T %a %D : Finished." >> /var/log/oracle
echo "-------------------------------------------------" >> /var/log/oracle
;;

'restart')
$0 stop
$0 start
;;
esac


ä¿å­˜é€€å‡º
æ”¹å˜æ–‡ä»¶æƒé™
[root@orcale ~]# chmod 755 /etc/init.d/oracle

æ·»åŠ æœåŠ¡
[root@orcale ~]# chkconfig --level 35 oracle on

éœ€è¦åœ¨å…³æœºæˆ–é‡å¯æœºå™¨ä¹‹å‰åœæ­¢æ•°æ®åº“ï¼Œåšä¸€ä¸‹æ“ä½œ
# ln -s /etc/init.d/oracle /etc/rc0.d/K01oracle   //å…³æœº
# ln -s /etc/init.d/oracle /etc/rc6.d/K01oracle   //é‡å¯ 

6. ä½¿ç”¨æ–¹æ³•
# service oracle start        //å¯åŠ¨oracle
# service oracle stop        //å…³é—­oracle
# service oracle restart     //é‡å¯oracle

å¼€æœºè‡ªå¯åŠ¨

é‡æ–°è¿æ¥DBeaver è½¯ä»¶ï¼Œæµ‹è¯•æˆåŠŸ




[root@orcale init.d]# vim oracle
[root@orcale init.d]# su - oracle
Last login: Sun May  2 02:39:02 EDT 2021 on pts/0
[oracle@orcale ~]$ chmod 755 /etc/init.d/oracle
chmod: changing permissions of â€˜/etc/init.d/oracleâ€™: Operation not permitted
[oracle@orcale ~]$ su root
Password:
[root@orcale oracle]# chmod 755 /etc/init.d/oracle
[root@orcale oracle]# chkconfig --level 35 oracle on
[root@orcale oracle]# service oracle stop
Stoping Oracle Listeners ...
Done.
Stoping Oracle Databases ...
Done.

[root@orcale oracle]#